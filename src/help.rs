use serenity::{
    client::Context,
    framework::standard::{
        macros::command,
        Args, CommandResult,
    },
    model::channel::Message,
};
use std::env;

#[command]
pub async fn help(ctx: &Context, msg: &Message, _args: Args) -> CommandResult {
    let _typing = ctx.http.start_typing(msg.channel_id.0)?;
    let prefix = env::var("PREFIX").unwrap_or_else(|_| "!".to_string());
    
    let response = format!(
        "**ğŸ¤– Meri Bot - Discord AI Assistant**\n\n\
        **ğŸ“‹ Basic Commands:**\n\
        â€¢ `{0}ping` - Test bot connectivity with response time\n\
        â€¢ `{0}echo <text>` - Echo your message\n\
        â€¢ `{0}help` - Show this help\n\n\
        **ğŸ–¼ï¸ Profile Picture:**\n\
        â€¢ `{0}ppfp @user` - Show user's profile picture\n\
        â€¢ **Aliases:** `{0}avatar`, `{0}pfp`, `{0}profilepic`\n\n\
        **ğŸ¤– AI Chat (LM Studio/Ollama):**

        â€¢ `{0}lm <prompt>` - AI chat with real-time streaming and **per-user conversation memory**
        â€¢ `{0}lm -s <query>` - AI-enhanced web search with embedded links (SerpAPI only)
        â€¢ `{0}lm --test` - Test connectivity to remote API server
        â€¢ `{0}lm --clear` - Clear your conversation history
        â€¢ **Aliases:** `{0}llm`, `{0}ai`, `{0}chat`

        **ğŸ§  AI Reasoning:**

        â€¢ `{0}reason <question>` - Specialized reasoning with real-time streaming, thinking tag filtering, and **per-user conversation memory** (using DeepSeek R1 model)
        â€¢ `{0}reason -s <query>` - Reasoning-enhanced analytical search with buffered chunking and <think> tag filtering (posts content in 2000-character chunks)
        â€¢ `{0}reason --clear` - Clear your reasoning conversation history
        â€¢ **Aliases:** `{0}reasoning`

        **ğŸ“„ Webpage Summarization:**

        â€¢ `{0}sum <url>` - Summarize webpage content using the reasoning model
        â€¢ **Aliases:** `{0}summarize`, `{0}webpage`
        â€¢ **Features:** HTML content extraction, intelligent summarization, automatic chunking for long summaries

        **ğŸ” Search Features:**\n\
        â€¢ **SerpAPI Integration:** Official search API with AI enhancement\n\
        â€¢ **AI Mode:** Direct search â†’ SerpAPI â†’ AI summary with embedded links\n\
        â€¢ Real-time progress updates and smart formatting\n\n\
        **âš¡ Advanced Features:**\n\
        â€¢ Real-time streaming responses (0.8s updates)\n\
        â€¢ Smart message chunking for long responses\n\
        â€¢ Thinking tag filtering for reasoning\n\
        â€¢ Buffered chunking for analytical search\n\
        â€¢ Multi-path configuration loading\n\
        â€¢ Comprehensive error handling\n\n\
        **ğŸ› ï¸ Setup:**\n\
        â€¢ **Required:** `botconfig.txt` with Discord token\n\
        â€¢ **AI Features:** `lmapiconf.txt` with LM Studio/Ollama config and SerpAPI key\n\
        â€¢ **Optional:** Custom prompts for search and reasoning\n\n\
        **ğŸš€ Quick Test:**\n\
        `{0}ping` (shows response time) â†’ `{0}lm -s rust tutorial` â†’ `{0}lm Hello!` â†’ `{0}reason Why is the sky blue?` â†’ `{0}sum https://example.com`\n\n\
        **ğŸ“š Full setup guide:** Check README.md for detailed instructions!", 
        prefix
    );
    
    msg.reply(ctx, &response).await?;
    Ok(())
} 