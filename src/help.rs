use serenity::{
    client::Context,
    framework::standard::{
        macros::command,
        Args, CommandResult,
    },
    model::channel::Message,
};
use std::env;

#[command]
pub async fn help(ctx: &Context, msg: &Message, _args: Args) -> CommandResult {
    let _typing = ctx.http.start_typing(msg.channel_id.0)?;
    let prefix = env::var("PREFIX").unwrap_or_else(|_| "!".to_string());
    
    let response = format!(
        "**ğŸ¤– Meri Bot - Discord AI Assistant**\n\n\
        **ğŸ“‹ Basic Commands:**\n\
        â€¢ `{0}ping` - Test bot connectivity with response time\n\
        â€¢ `{0}echo <text>` - Echo your message\n\
        â€¢ `{0}help` - Show this help\n\n\
        **ğŸ–¼ï¸ Profile Picture:**\n\
        â€¢ `{0}ppfp @user` - Show user's profile picture\n\
        â€¢ **Aliases:** `{0}avatar`, `{0}pfp`, `{0}profilepic`\n\n\
        **ğŸ¤– AI Chat (LM Studio/Ollama):**\n\
        â€¢ `{0}lm <prompt>` - AI chat with real-time streaming\n\
        â€¢ `{0}lm -s <query>` - AI-enhanced web search with embedded links\n\
        â€¢ **Aliases:** `{0}llm`, `{0}ai`, `{0}chat`\n\n\
        **ğŸ§  AI Reasoning:**\n\
        â€¢ `{0}reason <question>` - Specialized reasoning with thinking tag filtering\n\
        â€¢ `{0}reason -s <query>` - Reasoning-enhanced analytical search\n\
        â€¢ **Aliases:** `{0}reasoning`\n\n\
        **ğŸ” Search Features:**\n\
        â€¢ **AI Mode:** Direct search â†’ web search â†’ AI summary with embedded links\n\
        â€¢ **Basic Mode:** Direct DuckDuckGo search (no config needed)\n\
        â€¢ Real-time progress updates and smart formatting\n\n\
        **âš¡ Advanced Features:**\n\
        â€¢ Real-time streaming responses (0.8s updates)\n\
        â€¢ Smart message chunking for long responses\n\
        â€¢ Thinking tag filtering for reasoning\n\
        â€¢ Multi-path configuration loading\n\
        â€¢ Comprehensive error handling\n\n\
        **ğŸ› ï¸ Setup:**\n\
        â€¢ **Required:** `botconfig.txt` with Discord token\n\
        â€¢ **AI Features:** `lmapiconf.txt` with LM Studio/Ollama config\n\
        â€¢ **Optional:** Custom prompts for search and reasoning\n\n\
        **ğŸš€ Quick Test:**\n\
        `{0}ping` (shows response time) â†’ `{0}lm -s rust tutorial` â†’ `{0}lm Hello!` â†’ `{0}reason Why is the sky blue?`\n\n\
        **ğŸ“š Full setup guide:** Check README.md for detailed instructions!", 
        prefix
    );
    
    msg.reply(ctx, &response).await?;
    Ok(())
} 