// sum.rs - Self-Contained Webpage and YouTube Summarization Command Module
// This module implements the ^sum command, providing AI-powered summarization for webpages and YouTube videos.
// It supports robust content fetching, VTT/HTML cleaning, RAG chunking, and real-time streaming to Discord.
//
// Key Features:
// - Summarizes arbitrary webpages and YouTube videos
// - Uses yt-dlp for YouTube transcript extraction
// - Cleans and processes VTT/HTML content
// - RAG (map-reduce) chunking for long content
// - Real-time streaming of summary to Discord
// - Multi-path config and prompt loading
// - Robust error handling and logging
// - Self-contained with no external module dependencies
//
// Self-contained includes:
// - LM Studio configuration loading and management
// - HTTP client setup with connection pooling
// - Chat completion functionality with retry logic
// - All necessary structures and functions from search.rs and reason.rs

use serenity::{
    client::Context,
    framework::standard::{macros::command, macros::group, Args, CommandResult},
    model::channel::Message,
};
use std::time::Duration;
use std::fs;
use std::process::Command;
use uuid::Uuid;
use log::{info, warn, error, debug, trace};
use serde::{Deserialize, Serialize};
use regex::Regex;
use std::time::Instant;
use sha2::{Sha256, Digest};
use std::collections::HashMap;
use tokio::sync::OnceCell;

// ============================================================================
// SELF-CONTAINED COMPONENTS FROM SEARCH.RS AND REASON.RS
// ============================================================================

// Global HTTP client for connection pooling and reuse
static HTTP_CLIENT: OnceCell<reqwest::Client> = OnceCell::const_new();

// Initialize shared HTTP client with optimized settings
pub async fn get_http_client() -> &'static reqwest::Client {
    HTTP_CLIENT.get_or_init(|| async {
        reqwest::Client::builder()
            .timeout(Duration::from_secs(120)) // Increased timeout for LM Studio
            .connect_timeout(Duration::from_secs(30)) // Connection timeout
            .pool_idle_timeout(Duration::from_secs(90)) // Keep connections alive
            .pool_max_idle_per_host(10) // Connection pool size per host
            .danger_accept_invalid_certs(true) // Accept self-signed certificates for local servers
            .tcp_keepalive(Duration::from_secs(60)) // TCP keepalive
            .http2_keep_alive_interval(Duration::from_secs(30)) // HTTP/2 keepalive
            .http2_keep_alive_timeout(Duration::from_secs(10)) // HTTP/2 keepalive timeout
            .http2_keep_alive_while_idle(true) // Keep HTTP/2 alive when idle
            .user_agent("Meri-Bot-Rust-Client/1.0") // Identify the client
            .build()
            .expect("Failed to create HTTP client")
    }).await
}

// Chat message structure for context
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMessage {
    pub role: String,
    pub content: String,
}

// LM configuration structure
#[derive(Debug, Clone)]
pub struct LMConfig {
    pub base_url: String,
    pub timeout: u64,
    pub default_model: String,
    pub default_reason_model: String,
    pub default_summarization_model: String,
    pub default_ranking_model: String,
    pub default_temperature: f32,
    pub default_max_tokens: i32,
    pub max_discord_message_length: usize,
    pub response_format_padding: usize,
    pub default_vision_model: String,
    pub default_seed: Option<i64>, // Optional seed for reproducible responses
}

/// Enhanced connectivity test function
pub async fn test_api_connectivity(config: &LMConfig) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    let client = get_http_client().await;
    
    println!("[DEBUG][CONNECTIVITY] Testing API connectivity to: {}", config.base_url);
    
    // Test 1: Basic server connectivity
    let basic_response = client
        .get(&config.base_url)
        .timeout(Duration::from_secs(10))
        .send()
        .await;
    
    match basic_response {
        Ok(response) => {
            println!("[DEBUG][CONNECTIVITY] Basic connectivity OK - Status: {}", response.status());
        }
        Err(e) => {
            let error_msg = format!("{}", e);
            if error_msg.contains("os error 10013") || error_msg.contains("access permissions") {
                return Err(format!(
                    "üö´ **Windows Network Permission Error (10013)**\n\n\
                    Cannot connect to LM Studio at `{}`\n\n\
                    **Solutions:**\n\
                    ‚Ä¢ **Add Firewall Exception**: Windows Defender Firewall ‚Üí Allow an app ‚Üí Add this program\n\
                    ‚Ä¢ **Run as Administrator**: Try running the bot with administrator privileges\n\
                    ‚Ä¢ **Check LM Studio**: Ensure LM Studio is running and accessible\n\
                    ‚Ä¢ **Try localhost**: Use `http://127.0.0.1:1234` instead of `http://localhost:1234`\n\
                    ‚Ä¢ **Check Port**: Verify no other application is using the port\n\n\
                    **Original error:** {}", 
                    config.base_url, e
                ).into());
            } else if error_msg.contains("timeout") || error_msg.contains("timed out") {
                return Err(format!(
                    "‚è∞ **Connection Timeout**\n\n\
                    Cannot reach LM Studio server at `{}` within 10 seconds\n\n\
                    **Solutions:**\n\
                    ‚Ä¢ **Check LM Studio**: Ensure LM Studio is running and responsive\n\
                    ‚Ä¢ **Network Connection**: Verify your network connection is stable\n\
                    ‚Ä¢ **Server Load**: LM Studio might be overloaded - wait and retry\n\
                    ‚Ä¢ **Firewall**: Check if firewall is blocking the connection\n\n\
                    **Original error:** {}", 
                    config.base_url, e
                ).into());
            } else if error_msg.contains("refused") || error_msg.contains("connection refused") {
                return Err(format!(
                    "üö´ **Connection Refused**\n\n\
                    LM Studio at `{}` is not accepting connections\n\n\
                    **Solutions:**\n\
                    ‚Ä¢ **Start LM Studio**: Make sure LM Studio is running\n\
                    ‚Ä¢ **Check Port**: Verify LM Studio is listening on the correct port (usually 1234)\n\
                    ‚Ä¢ **Load Model**: Ensure a model is loaded in LM Studio\n\
                    ‚Ä¢ **Server Status**: Check LM Studio's server status indicator\n\
                    ‚Ä¢ **Alternative Port**: Try port 11434 if using Ollama instead\n\n\
                    **Original error:** {}", 
                    config.base_url, e
                ).into());
            } else if error_msg.contains("dns") || error_msg.contains("name resolution") {
                return Err(format!(
                    "üåê **DNS Resolution Error**\n\n\
                    Cannot resolve hostname in `{}`\n\n\
                    **Solutions:**\n\
                    ‚Ä¢ **Use IP Address**: Try `http://127.0.0.1:1234` instead of `http://localhost:1234`\n\
                    ‚Ä¢ **Check Hostname**: Verify the hostname is correct\n\
                    ‚Ä¢ **DNS Settings**: Check your DNS configuration\n\n\
                    **Original error:** {}", 
                    config.base_url, e
                ).into());
            } else {
                return Err(format!(
                    "üîó **Connection Error**\n\n\
                    Cannot connect to LM Studio at `{}`\n\n\
                    **Solutions:**\n\
                    ‚Ä¢ **Check URL**: Verify the base URL in lmapiconf.txt\n\
                    ‚Ä¢ **Start LM Studio**: Ensure LM Studio is running\n\
                    ‚Ä¢ **Network**: Check your network connection\n\
                    ‚Ä¢ **Firewall**: Verify firewall settings\n\n\
                    **Original error:** {}", 
                    config.base_url, e
                ).into());
            }
        }
    }
    
    // Test 2: API endpoint availability
    let api_url = format!("{}/v1/chat/completions", config.base_url);
    let test_payload = serde_json::json!({
        "model": config.default_model,
        "messages": [{"role": "user", "content": "test"}],
        "max_tokens": 1,
        "temperature": 0.1
    });
    
    println!("[DEBUG][CONNECTIVITY] Testing API endpoint: {}", api_url);
    
    let api_response = client
        .post(&api_url)
        .json(&test_payload)
        .timeout(Duration::from_secs(60)) // 1 minute for API endpoint test
        .send()
        .await;
    
    match api_response {
        Ok(response) => {
            let status = response.status();
            if status.is_success() || status == 400 || status == 422 {
                // 400/422 are acceptable - means API is responding but request format might be wrong
                println!("[DEBUG][CONNECTIVITY] API endpoint OK - Status: {}", status);
                return Ok(());
            } else if status == 404 {
                return Err(format!(
                    "üö´ **API Endpoint Not Found (404)**\n\n\
                    The endpoint `{}` was not found\n\n\
                    **Solutions:**\n\
                    ‚Ä¢ **Check LM Studio Version**: Ensure you're using a recent version that supports OpenAI API\n\
                    ‚Ä¢ **Enable API Server**: Make sure the 'Start Server' option is enabled in LM Studio\n\
                    ‚Ä¢ **Correct Port**: LM Studio usually uses port 1234, Ollama uses 11434\n\
                    ‚Ä¢ **API Path**: Verify the API path is `/v1/chat/completions`\n\n\
                    **Current URL:** {}", 
                    api_url, config.base_url
                ).into());
            } else {
                let error_text = response.text().await.unwrap_or_else(|_| "Unknown error".to_string());
                return Err(format!(
                    "üö´ **API Error (HTTP {})**\n\n\
                    LM Studio API returned an error\n\n\
                    **Response:** {}\n\n\
                    **Solutions:**\n\
                    ‚Ä¢ **Load Model**: Ensure a model is loaded in LM Studio\n\
                    ‚Ä¢ **Check Model Name**: Verify the model name in lmapiconf.txt matches loaded model\n\
                    ‚Ä¢ **Server Status**: Check LM Studio's status and logs\n\n\
                    **API URL:** {}", 
                    status, error_text, api_url
                ).into());
            }
        }
        Err(e) => {
            // API test failed, but basic connectivity worked, so this might be a model/configuration issue
            println!("[DEBUG][CONNECTIVITY] API test failed but basic connectivity OK: {}", e);
            return Err(format!(
                "‚ö†Ô∏è **API Configuration Issue**\n\n\
                Basic connectivity to `{}` works, but API test failed\n\n\
                **Likely Issues:**\n\
                ‚Ä¢ **Model Not Loaded**: No model is loaded in LM Studio\n\
                ‚Ä¢ **Wrong Model Name**: Model name in lmapiconf.txt doesn't match loaded model\n\
                ‚Ä¢ **API Not Enabled**: LM Studio server is not started\n\
                ‚Ä¢ **Version Issue**: LM Studio version doesn't support OpenAI API\n\n\
                **Error:** {}", 
                config.base_url, e
            ).into());
        }
    }
}

/// Load LM Studio/Ollama configuration from lmapiconf.txt file with enhanced validation
pub async fn load_lm_config() -> Result<LMConfig, Box<dyn std::error::Error + Send + Sync>> {
    // Trace-level function entry
    trace!("[TRACE][SUM][load_lm_config] === FUNCTION ENTRY ===");
    trace!("[TRACE][SUM][load_lm_config] Function: load_lm_config()");
    trace!("[TRACE][SUM][load_lm_config] Current working dir: {:?}", std::env::current_dir());
    
    let config_paths = [
        "lmapiconf.txt",
        "../lmapiconf.txt", 
        "../../lmapiconf.txt",
        "src/lmapiconf.txt"
    ];
    
    trace!("[TRACE][SUM][load_lm_config] Config paths to try: {:?}", config_paths);
    
    let mut config_content = String::new();
    let mut config_file_found = false;
    let mut config_file_path = "";
    
    // Try to read from multiple possible locations
    trace!("[TRACE][SUM][load_lm_config] === FILE SEARCH LOOP ===");
    for (index, path) in config_paths.iter().enumerate() {
        trace!("[TRACE][SUM][load_lm_config] Attempt {}: trying path '{}'", index + 1, path);
        trace!("[TRACE][SUM][load_lm_config] Path exists: {}", std::path::Path::new(path).exists());
        
        match fs::read_to_string(path) {
            Ok(content) => {
                trace!("[TRACE][SUM][load_lm_config] SUCCESS: File read from '{}'", path);
                trace!("[TRACE][SUM][load_lm_config] Content length: {} bytes", content.len());
                trace!("[TRACE][SUM][load_lm_config] Content preview: {}", &content[..std::cmp::min(200, content.len())]);
                
                config_content = content;
                config_file_found = true;
                config_file_path = path;
                println!("‚úÖ Configuration loaded from: {}", path);
                break;
            }
            Err(e) => {
                trace!("[TRACE][SUM][load_lm_config] FAILED: Could not read '{}': {}", path, e);
                continue;
            }
        }
    }
    
    trace!("[TRACE][SUM][load_lm_config] File search complete. Found: {}", config_file_found);
    
    if !config_file_found {
        return Err(format!(
            "‚ùå **Configuration File Not Found**\n\n\
            Could not find `lmapiconf.txt` in any of these locations:\n\
            ‚Ä¢ ./lmapiconf.txt\n\
            ‚Ä¢ ../lmapiconf.txt\n\
            ‚Ä¢ ../../lmapiconf.txt\n\
            ‚Ä¢ src/lmapiconf.txt\n\n\
            **Solution:** Copy `example_lmapiconf.txt` to `lmapiconf.txt` and configure it for your setup."
        ).into());
    }
    
    // Parse configuration
    let mut config_map = HashMap::new();
    for (line_num, line) in config_content.lines().enumerate() {
        let line = line.trim();
        if line.is_empty() || line.starts_with('#') {
            continue;
        }
        
        if let Some(equals_pos) = line.find('=') {
            let key = line[..equals_pos].trim().to_string();
            let value = line[equals_pos + 1..].trim().to_string();
            config_map.insert(key, value);
        } else {
            println!("‚ö†Ô∏è Warning: Invalid line {} in {}: {}", line_num + 1, config_file_path, line);
        }
    }
    
    // Validate required keys
    let required_keys = [
        "LM_STUDIO_BASE_URL",
        "LM_STUDIO_TIMEOUT",
        "DEFAULT_MODEL",
        "DEFAULT_REASON_MODEL",
        "DEFAULT_SUMMARIZATION_MODEL",
        "DEFAULT_RANKING_MODEL",
        "DEFAULT_TEMPERATURE",
        "DEFAULT_MAX_TOKENS",
        "MAX_DISCORD_MESSAGE_LENGTH",
        "RESPONSE_FORMAT_PADDING",
        "DEFAULT_VISION_MODEL",
    ];
    
    let mut missing_keys = Vec::new();
    for key in &required_keys {
        if !config_map.contains_key(*key) {
            missing_keys.push(*key);
        }
    }
    
    if !missing_keys.is_empty() {
        return Err(format!(
            "‚ùå **Missing Configuration Keys**\n\n\
            The following required keys are missing from `{}`:\n\
            {}\n\n\
            **Solution:** Add these keys to your lmapiconf.txt file. See example_lmapiconf.txt for reference.",
            config_file_path,
            missing_keys.iter().map(|k| format!("‚Ä¢ {}", k)).collect::<Vec<_>>().join("\n")
        ).into());
    }
    
    // Extract and validate configuration values
    let base_url = config_map.get("LM_STUDIO_BASE_URL")
        .ok_or("LM_STUDIO_BASE_URL not found in lmapiconf.txt")?
        .clone();
    
    // Validate base URL format
    if !base_url.starts_with("http://") && !base_url.starts_with("https://") {
        return Err(format!(
            "‚ùå **Invalid Base URL**\n\n\
            LM_STUDIO_BASE_URL must start with http:// or https://\n\
            Current value: `{}`\n\n\
            **Examples:**\n\
            ‚Ä¢ http://localhost:1234 (LM Studio)\n\
            ‚Ä¢ http://localhost:11434 (Ollama)\n\
            ‚Ä¢ http://127.0.0.1:1234 (Local IP)",
            base_url
        ).into());
    }
    
    let timeout = config_map.get("LM_STUDIO_TIMEOUT")
        .ok_or("LM_STUDIO_TIMEOUT not found in lmapiconf.txt")?
        .parse::<u64>()
        .map_err(|_| "LM_STUDIO_TIMEOUT must be a valid number (seconds)")?;
    
    if timeout == 0 || timeout > 600 {
        return Err(format!(
            "‚ùå **Invalid Timeout Value**\n\n\
            LM_STUDIO_TIMEOUT must be between 1 and 600 seconds\n\
            Current value: {} seconds\n\n\
            **Recommended:** 30-120 seconds",
            timeout
        ).into());
    }
    
    let default_model = config_map.get("DEFAULT_MODEL")
        .ok_or("DEFAULT_MODEL not found in lmapiconf.txt")?
        .clone();
        
    if default_model.trim().is_empty() {
        return Err("‚ùå DEFAULT_MODEL cannot be empty. Specify the model name loaded in LM Studio.".into());
    }
    
    let default_reason_model = config_map.get("DEFAULT_REASON_MODEL")
        .ok_or("DEFAULT_REASON_MODEL not found in lmapiconf.txt")?
        .clone();
        
    let default_summarization_model = config_map.get("DEFAULT_SUMMARIZATION_MODEL")
        .ok_or("DEFAULT_SUMMARIZATION_MODEL not found in lmapiconf.txt")?
        .clone();
        
    let default_ranking_model = config_map.get("DEFAULT_RANKING_MODEL")
        .ok_or("DEFAULT_RANKING_MODEL not found in lmapiconf.txt")?
        .clone();
        
    let default_vision_model = config_map.get("DEFAULT_VISION_MODEL")
        .ok_or("DEFAULT_VISION_MODEL not found in lmapiconf.txt")?
        .clone();
    
    let default_temperature = config_map.get("DEFAULT_TEMPERATURE")
        .ok_or("DEFAULT_TEMPERATURE not found in lmapiconf.txt")?
        .parse::<f32>()
        .map_err(|_| "DEFAULT_TEMPERATURE must be a valid number")?;
    
    if default_temperature < 0.0 || default_temperature > 2.0 {
        return Err(format!(
            "‚ùå **Invalid Temperature Value**\n\n\
            DEFAULT_TEMPERATURE must be between 0.0 and 2.0\n\
            Current value: {}\n\n\
            **Recommended:** 0.1-1.0 (0.7-0.8 is typical)",
            default_temperature
        ).into());
    }
    
    let default_max_tokens = config_map.get("DEFAULT_MAX_TOKENS")
        .ok_or("DEFAULT_MAX_TOKENS not found in lmapiconf.txt")?
        .parse::<i32>()
        .map_err(|_| "DEFAULT_MAX_TOKENS must be a valid number")?;
    
    if default_max_tokens <= 0 || default_max_tokens > 32768 {
        return Err(format!(
            "‚ùå **Invalid Max Tokens Value**\n\n\
            DEFAULT_MAX_TOKENS must be between 1 and 32768\n\
            Current value: {}\n\n\
            **Recommended:** 1000-8000 for most use cases",
            default_max_tokens
        ).into());
    }
    
    let max_discord_message_length = config_map.get("MAX_DISCORD_MESSAGE_LENGTH")
        .ok_or("MAX_DISCORD_MESSAGE_LENGTH not found in lmapiconf.txt")?
        .parse::<usize>()
        .map_err(|_| "MAX_DISCORD_MESSAGE_LENGTH must be a valid number")?;
    
    let response_format_padding = config_map.get("RESPONSE_FORMAT_PADDING")
        .ok_or("RESPONSE_FORMAT_PADDING not found in lmapiconf.txt")?
        .parse::<usize>()
        .map_err(|_| "RESPONSE_FORMAT_PADDING must be a valid number")?;
    
    // Optional seed configuration for reproducible responses
    let default_seed = config_map.get("DEFAULT_SEED")
        .filter(|s| !s.trim().is_empty()) // Ignore empty values
        .map(|s| s.parse::<i64>())
        .transpose()
        .map_err(|_| "DEFAULT_SEED must be a valid integer if specified")?;
    
    let config = LMConfig {
        base_url,
        timeout,
        default_model,
        default_reason_model,
        default_summarization_model,
        default_ranking_model,
        default_temperature,
        default_max_tokens,
        max_discord_message_length,
        response_format_padding,
        default_vision_model,
        default_seed,
    };
    
    // Test connectivity after loading configuration
    println!("üîç Testing API connectivity...");
    if let Err(e) = test_api_connectivity(&config).await {
        return Err(format!(
            "‚ùå **Connectivity Test Failed**\n\n\
            Configuration loaded successfully from `{}`, but connectivity test failed:\n\n\
            {}\n\n\
            **Config Details:**\n\
            ‚Ä¢ Base URL: {}\n\
            ‚Ä¢ Default Model: {}\n\
            ‚Ä¢ Timeout: {}s",
            config_file_path, e, config.base_url, config.default_model, config.timeout
        ).into());
    }
    
    println!("‚úÖ API connectivity test passed!");
    Ok(config)
}

/// Enhanced chat completion with retry logic and better error handling
pub async fn chat_completion(
    messages: Vec<ChatMessage>,
    model: &str,
    config: &LMConfig,
    max_tokens: Option<i32>,
) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
    // Trace-level function entry for chat completion
    trace!("[TRACE][SUM][chat_completion] === FUNCTION ENTRY ===");
    trace!("[TRACE][SUM][chat_completion] Model: '{}'", model);
    trace!("[TRACE][SUM][chat_completion] Messages count: {}", messages.len());
    trace!("[TRACE][SUM][chat_completion] Max tokens: {:?}", max_tokens);
    trace!("[TRACE][SUM][chat_completion] Config base URL: {}", config.base_url);
    trace!("[TRACE][SUM][chat_completion] Config temperature: {}", config.default_temperature);
    
    let result = chat_completion_with_retries(messages, model, config, max_tokens, 3).await;
    
    // Trace-level function exit
    match &result {
        Ok(response) => {
            trace!("[TRACE][SUM][chat_completion] === FUNCTION EXIT (SUCCESS) ===");
            trace!("[TRACE][SUM][chat_completion] Response length: {} chars", response.len());
        }
        Err(e) => {
            trace!("[TRACE][SUM][chat_completion] === FUNCTION EXIT (ERROR) ===");
            trace!("[TRACE][SUM][chat_completion] Error: {}", e);
        }
    }
    
    result
}

/// Chat completion with configurable retry attempts
async fn chat_completion_with_retries(
    messages: Vec<ChatMessage>,
    model: &str,
    config: &LMConfig,
    max_tokens: Option<i32>,
    max_retries: u32,
) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
    let client = get_http_client().await;
    let api_url = format!("{}/v1/chat/completions", config.base_url);
    
    let chat_request = serde_json::json!({
        "model": model,
        "messages": messages,
        "temperature": config.default_temperature,
        "max_tokens": max_tokens.unwrap_or(config.default_max_tokens),
        "stream": false,
        "seed": config.default_seed
    });

    let mut last_error: Option<Box<dyn std::error::Error + Send + Sync>> = None;
    
    for attempt in 1..=max_retries {
        println!("[DEBUG][CHAT] Attempt {}/{} - Sending request to: {}", attempt, max_retries, api_url);
        
        let start_time = std::time::Instant::now();
        
        let response = match client
            .post(&api_url)
            .json(&chat_request)
            .timeout(Duration::from_secs(config.timeout))
            .send()
            .await
        {
            Ok(resp) => {
                let elapsed = start_time.elapsed();
                println!("[DEBUG][CHAT] Request completed in {:.2}s - Status: {}", elapsed.as_secs_f32(), resp.status());
                resp
            }
            Err(e) => {
                let elapsed = start_time.elapsed();
                println!("[DEBUG][CHAT] Request failed after {:.2}s: {}", elapsed.as_secs_f32(), e);
                
                let error_msg = format!("{}", e);
                
                // Check for specific error types that might benefit from retry
                let should_retry = attempt < max_retries && (
                    error_msg.contains("timeout") ||
                    error_msg.contains("connection reset") ||
                    error_msg.contains("connection aborted") ||
                    error_msg.contains("broken pipe") ||
                    error_msg.contains("connection closed")
                );
                
                if should_retry {
                    let delay = Duration::from_millis(1000 * attempt as u64); // Exponential backoff
                    println!("[DEBUG][CHAT] Retrying in {:.1}s...", delay.as_secs_f32());
                    tokio::time::sleep(delay).await;
                    last_error = Some(Box::new(std::io::Error::new(std::io::ErrorKind::Other, format!("{}", e))));
                    continue;
                } else {
                    // Don't retry for these errors - they're likely configuration issues
                    if error_msg.contains("os error 10013") || error_msg.contains("access permissions") {
                        return Err(format!(
                            "üö´ **Windows Network Permission Error**\n\n\
                            Cannot connect to LM Studio API\n\n\
                            **Quick Fixes:**\n\
                            ‚Ä¢ **Run as Administrator**: Right-click and 'Run as administrator'\n\
                            ‚Ä¢ **Windows Firewall**: Add firewall exception for this program\n\
                            ‚Ä¢ **Try localhost**: Use `http://127.0.0.1:1234` in lmapiconf.txt\n\n\
                            **Current URL:** {}\n\
                            **Error:** {}", 
                            config.base_url, e
                        ).into());
                    } else if error_msg.contains("refused") || error_msg.contains("connection refused") {
                        return Err(format!(
                            "üö´ **Connection Refused**\n\n\
                            LM Studio is not accepting connections\n\n\
                            **Solutions:**\n\
                            ‚Ä¢ **Start LM Studio**: Make sure LM Studio is running\n\
                            ‚Ä¢ **Load Model**: Ensure a model is loaded\n\
                            ‚Ä¢ **Enable Server**: Click 'Start Server' in LM Studio\n\
                            ‚Ä¢ **Check Port**: Verify port 1234 is available\n\n\
                            **Current URL:** {}\n\
                            **Error:** {}", 
                            config.base_url, e
                        ).into());
                    } else {
                        return Err(format!("API request failed: {}", e).into());
                    }
                }
            }
        };

        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await.unwrap_or_else(|_| "Unable to read error response".to_string());
            
            // Check if this is a retryable server error
            let is_server_error = status.is_server_error();
            let should_retry = attempt < max_retries && is_server_error;
            
            if should_retry {
                let delay = Duration::from_millis(1000 * attempt as u64);
                println!("[DEBUG][CHAT] Server error ({}), retrying in {:.1}s...", status, delay.as_secs_f32());
                tokio::time::sleep(delay).await;
                last_error = Some(Box::new(std::io::Error::new(std::io::ErrorKind::Other, format!("HTTP {} - {}", status, error_text))));
                continue;
            } else {
                return Err(format!(
                    "üö´ **API Error (HTTP {})**\n\n\
                    **Response:** {}\n\n\
                    **Solutions:**\n\
                    ‚Ä¢ **Model Loaded**: Ensure model '{}' is loaded in LM Studio\n\
                    ‚Ä¢ **Model Name**: Verify model name matches exactly\n\
                    ‚Ä¢ **Server Status**: Check LM Studio server logs\n\
                    ‚Ä¢ **Memory**: Ensure sufficient RAM for the model\n\n\
                    **API URL:** {}", 
                    status, error_text, model, api_url
                ).into());
            }
        }

        // Parse successful response
        let response_text = match response.text().await {
            Ok(text) => text,
            Err(e) => {
                if attempt < max_retries {
                    let delay = Duration::from_millis(1000 * attempt as u64);
                    println!("[DEBUG][CHAT] Failed to read response, retrying in {:.1}s...", delay.as_secs_f32());
                    tokio::time::sleep(delay).await;
                    last_error = Some(Box::new(e));
                    continue;
                } else {
                    return Err(format!("Failed to read response: {}", e).into());
                }
            }
        };
        
        let response_json: serde_json::Value = match serde_json::from_str(&response_text) {
            Ok(json) => json,
            Err(e) => {
                return Err(format!(
                    "üö´ **Invalid API Response**\n\n\
                    Failed to parse JSON response from LM Studio\n\n\
                    **Response:** {}\n\
                    **Parse Error:** {}\n\n\
                    **Solutions:**\n\
                    ‚Ä¢ **Update LM Studio**: Ensure you're using a recent version\n\
                    ‚Ä¢ **Check Model**: Verify the model supports chat completions\n\
                    ‚Ä¢ **Server Logs**: Check LM Studio logs for errors",
                    response_text.chars().take(500).collect::<String>(), e
                ).into());
            }
        };
        
        // Extract content from response
        if let Some(choices) = response_json["choices"].as_array() {
            if let Some(first_choice) = choices.get(0) {
                if let Some(message) = first_choice["message"].as_object() {
                    if let Some(content) = message["content"].as_str() {
                        let result = content.trim().to_string();
                        println!("[DEBUG][CHAT] Success! Generated {} characters", result.len());
                        return Ok(result);
                    }
                }
            }
        }
        
        // If we reach here, the JSON structure was unexpected
        return Err(format!(
            "üö´ **Unexpected API Response Format**\n\n\
            LM Studio returned a valid JSON response, but the structure was unexpected\n\n\
            **Response:** {}\n\n\
            **Solutions:**\n\
            ‚Ä¢ **Update LM Studio**: Ensure compatibility with OpenAI API format\n\
            ‚Ä¢ **Check Model**: Verify the model supports chat completions\n\
            ‚Ä¢ **API Version**: Ensure you're using a compatible API version",
            serde_json::to_string_pretty(&response_json).unwrap_or_else(|_| "Unable to format response".to_string())
        ).into());
    }
    
    // All retries exhausted
    Err(format!("Request failed after {} attempts. Last error: {}", 
                max_retries, 
                last_error.map(|e| format!("{}", e)).unwrap_or_else(|| "Unknown error".to_string())
    ).into())
}

// ============================================================================
// ORIGINAL SUM.RS FUNCTIONALITY
// ============================================================================

// SSE response structures for streaming summary
// Used to parse streaming JSON chunks from the AI API
#[derive(Deserialize)]
struct StreamResponse {
    choices: Vec<StreamChoice>, // Streaming choices (delta content)
}

#[derive(Deserialize)]
struct StreamChoice {
    delta: StreamDelta,         // Streaming delta (content chunk)
    finish_reason: Option<String>, // Reason for stream completion
}

#[derive(Deserialize)]
struct StreamDelta {
    content: Option<String>,    // Content chunk
}

#[command]
#[aliases("summarize", "webpage")]
/// Main ^sum command handler
/// Handles summarization of webpages and YouTube videos
/// Supports:
///   - ^sum <url> (webpage or YouTube)
pub async fn sum(ctx: &Context, msg: &Message, args: Args) -> CommandResult {
    let start_time = std::time::Instant::now();
    let command_uuid = Uuid::new_v4();
    
    // Trace-level function entry logging
    trace!("[TRACE][SUM] === FUNCTION ENTRY: sum() ===");
    trace!("[TRACE][SUM] Function: sum(), UUID: {}", command_uuid);
    trace!("[TRACE][SUM] Entry timestamp: {:?}", start_time);
    trace!("[TRACE][SUM] Context data lock acquired successfully");
    trace!("[TRACE][SUM] Message author: {} (ID: {})", msg.author.name, msg.author.id);
    trace!("[TRACE][SUM] Channel: {} (ID: {})", msg.channel_id, msg.channel_id.0);
    trace!("[TRACE][SUM] Guild: {:?}", msg.guild_id);
    trace!("[TRACE][SUM] Raw arguments: '{}'", args.message());
    
    info!("üì∫ === SUM COMMAND STARTED ===");
    info!("üÜî Command UUID: {}", command_uuid);
    info!("üë§ User: {} ({})", msg.author.name, msg.author.id);
    info!("üì∫ Channel: {} ({})", msg.channel_id, msg.channel_id.0);
    info!("üì∫ Guild: {:?}", msg.guild_id);
    info!("üì∫ Message ID: {}", msg.id);
    info!("üì∫ Timestamp: {:?}", msg.timestamp);
    
    // Enhanced logging for web page summarization debugging
    log::info!("üîç === SUM COMMAND DEBUG INFO ===");
    log::info!("üîç Command UUID: {}", command_uuid);
    log::info!("üîç User ID: {}", msg.author.id);
    log::info!("üîç Channel ID: {}", msg.channel_id);
    log::info!("üîç Message ID: {}", msg.id);
    log::info!("üîç Arguments: '{}'", args.message());
    log::info!("üîç Arguments length: {} characters", args.message().len());
    log::info!("üîç Start time: {:?}", start_time);
    
    debug!("üîß === COMMAND INITIALIZATION ===");
    debug!("üîß Command arguments: '{}'", args.message());
    debug!("üîß Arguments length: {} characters", args.message().len());
    debug!("üîß Arguments trimmed: '{}'", args.message().trim());
    debug!("üîß Arguments trimmed length: {} characters", args.message().trim().len());
    trace!("üîç Command initialization details: uuid={}, author_id={}, channel_id={}, message_id={}", 
           command_uuid, msg.author.id, msg.channel_id, msg.id);
    
    let url = args.message().trim();
    
    // Trace-level URL processing
    trace!("[TRACE][SUM] === URL PROCESSING ENTRY ===");
    trace!("[TRACE][SUM] Raw args message: '{}'", args.message());
    trace!("[TRACE][SUM] Raw args length: {} chars", args.message().len());
    trace!("[TRACE][SUM] After trim: '{}'", url);
    trace!("[TRACE][SUM] After trim length: {} chars", url.len());
    trace!("[TRACE][SUM] Is empty after trim: {}", url.is_empty());
    trace!("[TRACE][SUM] Contains http://: {}", url.contains("http://"));
    trace!("[TRACE][SUM] Contains https://: {}", url.contains("https://"));
    trace!("[TRACE][SUM] First 50 chars: '{}'", &url[..std::cmp::min(50, url.len())]);
    
    debug!("üîó === URL PROCESSING ===");
    debug!("üîó Raw URL: '{}'", url);
    debug!("üîó URL length: {} characters", url.len());
    debug!("üîó URL is empty: {}", url.is_empty());
    trace!("üîç URL processing: raw_length={}, trimmed_length={}, is_empty={}", 
           args.message().len(), url.len(), url.is_empty());
    
    // Logging is now configured globally in main.rs to show all levels
    debug!("üîß Logging configured for maximum debugging detail");
    trace!("üîç TRACE logging enabled - will show all function calls and data flow");
    
    if url.is_empty() {
        warn!("‚ùå === EMPTY URL ERROR ===");
        warn!("‚ùå Empty URL provided by user {} ({})", msg.author.name, msg.author.id);
        debug!("üîç URL validation failed: empty string");
        debug!("üîç Sending error message to user");
        trace!("üîç Empty URL error: user_id={}, channel_id={}, command_uuid={}", 
               msg.author.id, msg.channel_id, command_uuid);
        
        // Trace-level error exit
        trace!("[TRACE][SUM] === FUNCTION EXIT: sum() (EMPTY URL ERROR) ===");
        trace!("[TRACE][SUM] Function: sum(), UUID: {}", command_uuid);
        trace!("[TRACE][SUM] Exit status: ERROR - Empty URL");
        trace!("[TRACE][SUM] Exit timestamp: {:?}", std::time::Instant::now());
        
        msg.reply(ctx, "Please provide a URL to summarize!\n\n**Usage:** `^sum <url>`").await?;
        debug!("‚úÖ Error message sent successfully");
        return Ok(());
    }
    
    debug!("üîç === URL VALIDATION ===");
    debug!("üîç Validating URL format: {}", url);
    debug!("üîç URL starts with http://: {}", url.starts_with("http://"));
    debug!("üîç URL starts with https://: {}", url.starts_with("https://"));
    debug!("üîç URL contains youtube.com: {}", url.contains("youtube.com"));
    debug!("üîç URL contains youtu.be: {}", url.contains("youtu.be"));
    trace!("üîç URL validation details: starts_with_http={}, starts_with_https={}, contains_youtube_com={}, contains_youtu_be={}", 
           url.starts_with("http://"), url.starts_with("https://"), url.contains("youtube.com"), url.contains("youtu.be"));
    
    if !url.starts_with("http://") && !url.starts_with("https://") {
        warn!("‚ùå === INVALID URL FORMAT ERROR ===");
        warn!("‚ùå Invalid URL format provided: {}", url);
        debug!("üîç URL validation failed: missing http/https prefix");
        debug!("üîç URL first 10 characters: '{}'", url.chars().take(10).collect::<String>());
        trace!("üîç URL validation failure details: length={}, first_chars={}, command_uuid={}", 
               url.len(), url.chars().take(10).collect::<String>(), command_uuid);
        
        // Trace-level error exit
        trace!("[TRACE][SUM] === FUNCTION EXIT: sum() (INVALID URL FORMAT ERROR) ===");
        trace!("[TRACE][SUM] Function: sum(), UUID: {}", command_uuid);
        trace!("[TRACE][SUM] Exit status: ERROR - Invalid URL format");
        trace!("[TRACE][SUM] Invalid URL: '{}'", url);
        trace!("[TRACE][SUM] Exit timestamp: {:?}", std::time::Instant::now());
        
        msg.reply(ctx, "Please provide a valid URL starting with `http://` or `https://`").await?;
        debug!("‚úÖ Invalid URL error message sent");
        return Ok(());
    }
    debug!("‚úÖ URL format validation passed");
    trace!("üîç URL validation success: protocol={}, command_uuid={}", 
           if url.starts_with("https://") { "https" } else { "http" }, command_uuid);
    
    // Load LM configuration from lmapiconf.txt
    trace!("[TRACE][SUM] === CONFIGURATION LOADING ENTRY ===");
    trace!("[TRACE][SUM] About to call load_lm_config()");
    trace!("[TRACE][SUM] Current working directory: {:?}", std::env::current_dir());
    trace!("[TRACE][SUM] Command UUID: {}", command_uuid);
    
    debug!("üîß === CONFIGURATION LOADING ===");
    debug!("üîß Loading LM configuration from lmapiconf.txt...");
    trace!("üîç Configuration loading phase started: command_uuid={}", command_uuid);
    
    let config = match load_lm_config().await {
        Ok(cfg) => {
            info!("‚úÖ === CONFIGURATION LOADED SUCCESSFULLY ===");
            info!("‚úÖ LM configuration loaded successfully");
            debug!("üß† Using default model: {}", cfg.default_model);
            debug!("üß† Using reasoning model: {}", cfg.default_reason_model);
            debug!("üß† Using summarization model: {}", cfg.default_summarization_model);
            debug!("üåê API endpoint: {}", cfg.base_url);
            debug!("‚è±Ô∏è Timeout setting: {} seconds", cfg.timeout);
            debug!("üî• Temperature setting: {}", cfg.default_temperature);
            debug!("üìù Max tokens setting: {}", cfg.default_max_tokens);
            debug!("üìè Max Discord message length: {}", cfg.max_discord_message_length);
            debug!("üìè Response format padding: {}", cfg.response_format_padding);
            trace!("üîç Configuration details: max_discord_length={}, response_format_padding={}, command_uuid={}", 
                   cfg.max_discord_message_length, cfg.response_format_padding, command_uuid);
            cfg
        },
        Err(e) => {
            error!("‚ùå === CONFIGURATION LOADING ERROR ===");
            error!("‚ùå Failed to load LM configuration: {}", e);
            debug!("üîç Configuration loading error details: {:?}", e);
            debug!("üîç Configuration error type: {:?}", std::any::type_name_of_val(&e));
            trace!("üîç Configuration error: error_type={}, command_uuid={}", 
                   std::any::type_name_of_val(&e), command_uuid);
            msg.reply(ctx, &format!("‚ùå **Configuration Error**\n\n{}\n\n**Setup required:** Ensure `lmapiconf.txt` is properly configured with your LM Studio settings.", e)).await?;
            debug!("‚úÖ Configuration error message sent");
            return Ok(());
        }
    };
    
    debug!("üîß Configuration loaded successfully, proceeding with next steps");
    trace!("üîç Configuration phase completed: command_uuid={}", command_uuid);
    

    
    // Trace-level URL type detection
    trace!("[TRACE][SUM] === URL TYPE DETECTION ENTRY ===");
    trace!("[TRACE][SUM] URL to analyze: '{}'", url);
    trace!("[TRACE][SUM] URL length: {} chars", url.len());
    trace!("[TRACE][SUM] Checking youtube.com/...");
    let contains_youtube_com = url.contains("youtube.com/");
    trace!("[TRACE][SUM] Contains youtube.com/: {}", contains_youtube_com);
    trace!("[TRACE][SUM] Checking youtu.be/...");
    let contains_youtu_be = url.contains("youtu.be/");
    trace!("[TRACE][SUM] Contains youtu.be/: {}", contains_youtu_be);
    let is_youtube = contains_youtube_com || contains_youtu_be;
    trace!("[TRACE][SUM] Final determination - is_youtube: {}", is_youtube);
    trace!("[TRACE][SUM] Content type will be: {}", if is_youtube { "YouTube video" } else { "Webpage" });
    
    debug!("üîç === URL TYPE DETECTION ===");
    debug!("üîç Detecting URL type...");
    debug!("üîç URL contains youtube.com/: {}", url.contains("youtube.com/"));
    debug!("üîç URL contains youtu.be/: {}", url.contains("youtu.be/"));
    debug!("üîç Final YouTube detection: {}", is_youtube);
    trace!("üîç URL type detection details: contains_youtube_com={}, contains_youtu_be={}, is_youtube={}, command_uuid={}", 
           url.contains("youtube.com/"), url.contains("youtu.be/"), is_youtube, command_uuid);
    info!("üéØ === CONTENT TYPE DETECTED ===");
    info!("üéØ Processing {} URL: {}", if is_youtube { "YouTube" } else { "webpage" }, url);
    debug!("üìä URL type detection: YouTube = {}", is_youtube);
    
    // Always use the summarization model for all content types due to 32K context window
    let selected_model = &config.default_summarization_model;
    
    // Log the model selection
    info!("üéØ === SUMMARIZATION MODEL SELECTION ===");
    info!("üéØ Using summarization model for all content types: {}", selected_model);
    info!("üéØ Reason: 32K context window for optimal summarization performance");
    debug!("üéØ Model selection: summarization_model={}, content_type={}", selected_model, if is_youtube { "YouTube" } else { "webpage" });
    trace!("üîç Model selection: model={}, content_type={}, command_uuid={}", selected_model, if is_youtube { "youtube" } else { "webpage" }, command_uuid);
    
    // Create response message
    debug!("üí¨ === DISCORD MESSAGE CREATION ===");
    debug!("üí¨ Creating initial Discord response message...");
    trace!("üîç Discord message creation: author={}, channel={}, command_uuid={}", msg.author.name, msg.channel_id, command_uuid);
    let mut response_msg = msg.reply(ctx, "üîÑ Fetching content...").await?;
    debug!("‚úÖ Initial Discord message sent successfully");
    debug!("üìù Response message ID: {}", response_msg.id);
    debug!("üìù Response message channel ID: {}", response_msg.channel_id);
    debug!("üìù Response message content: '{}'", response_msg.content);
    trace!("üîç Discord message details: id={}, channel_id={}, content_length={}, command_uuid={}", 
           response_msg.id, response_msg.channel_id, response_msg.content.len(), command_uuid);
    
    // Add a small delay to avoid rate limiting if multiple requests are made quickly
    debug!("‚è≥ === RATE LIMITING DELAY ===");
    debug!("‚è≥ Adding 1-second delay to prevent rate limiting...");
    trace!("üîç Rate limiting delay: 1000ms, command_uuid={}", command_uuid);
    tokio::time::sleep(tokio::time::Duration::from_millis(1000)).await;
    debug!("‚úÖ Delay completed");
    trace!("üîç Rate limiting delay completed: command_uuid={}", command_uuid);
    
    // Fetch content
    info!("üåê === CONTENT FETCHING PHASE ===");
    info!("üåê Starting content fetching process...");
    debug!("üöÄ Content fetching phase initiated");
    trace!("üîç Content fetching phase: url_type={}, url={}, command_uuid={}", 
           if is_youtube { "youtube" } else { "webpage" }, url, command_uuid);

    let (subtitle_file_path, content) = if is_youtube {
        debug!("üé• === YOUTUBE CONTENT FETCHING ===");
        debug!("üé• YouTube URL detected, starting transcript extraction...");
        trace!("üîç YouTube transcript extraction started: command_uuid={}", command_uuid);
        match fetch_youtube_transcript(url).await {
            Ok(path) => {
                info!("‚úÖ === YOUTUBE TRANSCRIPT SUCCESS ===");
                info!("‚úÖ YouTube subtitle file created successfully: {}", path);
                debug!("üìÅ Subtitle file path: {}", path);
                debug!("üìÅ Subtitle file exists: {}", std::path::Path::new(&path).exists());
                trace!("üîç YouTube subtitle file success: path={}, command_uuid={}", path, command_uuid);
                
                // Read the subtitle file content for statistics only (RAG will handle the actual processing)
                debug!("üìñ === SUBTITLE FILE READING FOR STATISTICS ===");
                debug!("üìñ Reading subtitle file for statistics only...");
                match fs::read_to_string(&path) {
                    Ok(file_content) => {
                        debug!("üìñ Subtitle file read successfully: {} characters", file_content.len());
                        debug!("üìñ File content preview: {}", &file_content[..std::cmp::min(200, file_content.len())]);
                        trace!("üîç Subtitle file read: path={}, length={}, command_uuid={}", path, file_content.len(), command_uuid);
                        
                        let cleaned_content = clean_vtt_content(&file_content);
                        debug!("üßπ === VTT CLEANING FOR STATISTICS ===");
                        debug!("üßπ Cleaning VTT content for statistics only...");
                        debug!("üìù Original subtitle content: {} characters", file_content.len());
                        debug!("üìù Cleaned subtitle content: {} characters", cleaned_content.len());
                        debug!("üìù Content preview: {}", &cleaned_content[..std::cmp::min(200, cleaned_content.len())]);
                        debug!("üìä Subtitle statistics: {} characters, {} words", cleaned_content.len(), cleaned_content.split_whitespace().count());
                        debug!("üìÅ RAG will process the original file: {}", path);
                        trace!("üîç VTT cleaning for statistics: original_length={}, cleaned_length={}, word_count={}, command_uuid={}", 
                               file_content.len(), cleaned_content.len(), cleaned_content.split_whitespace().count(), command_uuid);
                        
                        // Store statistics for logging purposes only
                        debug!("üìä YouTube subtitle statistics: {} characters, {} words", 
                               cleaned_content.len(), cleaned_content.split_whitespace().count());
                    },
                    Err(e) => {
                        warn!("‚ö†Ô∏è === SUBTITLE FILE READ ERROR ===");
                        warn!("‚ö†Ô∏è Could not read subtitle file for statistics: {}", e);
                        debug!("üîç Subtitle file read error: path={}, error={}", path, e);
                        trace!("üîç Subtitle file read error: path={}, error_type={}, command_uuid={}", 
                               path, std::any::type_name_of_val(&e), command_uuid);
                    }
                }
                (Some(path), String::new()) // Empty content for YouTube since RAG handles the file
            },
            Err(e) => {
                error!("‚ùå === YOUTUBE TRANSCRIPT ERROR ===");
                error!("‚ùå Failed to fetch YouTube transcript: {}", e);
                debug!("üîç YouTube transcript error details: {:?}", e);
                debug!("üîç YouTube transcript error type: {:?}", std::any::type_name_of_val(&e));
                trace!("üîç YouTube transcript error: error_type={}, command_uuid={}", 
                       std::any::type_name_of_val(&e), command_uuid);
                response_msg.edit(ctx, |m| {
                    m.content(format!("‚ùå Failed to fetch YouTube transcript: {}", e))
                }).await?;
                debug!("‚úÖ YouTube transcript error message sent to Discord");
                return Ok(());
            }
        }
    } else {
        // Content will be assigned from webpage processing
        debug!("üåê === WEBPAGE CONTENT FETCHING ===");
        debug!("üåê Webpage URL detected, starting content extraction...");
        trace!("üîç Webpage content extraction started: command_uuid={}", command_uuid);
        
        // Enhanced logging for web page processing
        log::info!("üåê === WEBPAGE PROCESSING STARTED ===");
        log::info!("üåê URL: {}", url);
        log::info!("üåê Command UUID: {}", command_uuid);
        log::info!("üåê Processing type: HTML file download and RAG processing");
        
        match fetch_webpage_content(url).await {
            Ok((page_content, html_file_path)) => {
                info!("‚úÖ === WEBPAGE CONTENT SUCCESS ===");
                info!("‚úÖ Webpage content fetched successfully: {} characters", page_content.len());
                info!("üíæ HTML file saved for RAG processing: {}", html_file_path);
                debug!("üìÑ Content preview: {}", &page_content[..std::cmp::min(200, page_content.len())]);
                debug!("üìä Webpage statistics: {} characters, {} words", page_content.len(), page_content.split_whitespace().count());
                debug!("üíæ HTML file path: {}", html_file_path);
                trace!("üîç Webpage content success: length={}, word_count={}, preview_chars={}, file_path={}, command_uuid={}", 
                       page_content.len(), page_content.split_whitespace().count(), std::cmp::min(200, page_content.len()), html_file_path, command_uuid);
                
                // Enhanced logging for successful web page processing
                log::info!("‚úÖ === WEBPAGE CONTENT SUCCESS DETAILS ===");
                log::info!("‚úÖ Content length: {} characters", page_content.len());
                log::info!("‚úÖ Word count: {} words", page_content.split_whitespace().count());
                log::info!("‚úÖ HTML file path: {}", html_file_path);
                log::info!("‚úÖ File exists: {}", std::path::Path::new(&html_file_path).exists());
                log::info!("‚úÖ Content preview: {}", &page_content[..std::cmp::min(300, page_content.len())]);
                log::info!("‚úÖ Processing will use RAG with file: {}", html_file_path);
                
                (Some(html_file_path), page_content)
            },
            Err(e) => {
                error!("‚ùå === WEBPAGE CONTENT ERROR ===");
                error!("‚ùå Failed to fetch webpage content: {}", e);
                debug!("üîç Webpage content error details: {:?}", e);
                debug!("üîç Webpage content error type: {:?}", std::any::type_name_of_val(&e));
                trace!("üîç Webpage content error: error_type={}, command_uuid={}", 
                       std::any::type_name_of_val(&e), command_uuid);
                response_msg.edit(ctx, |m| {
                    m.content(format!("‚ùå Failed to fetch webpage: {}", e))
                }).await?;
                debug!("‚úÖ Webpage content error message sent to Discord");
                return Ok(());
            }
        }
    };
    
    // Update status
    debug!("üìù === DISCORD MESSAGE UPDATE ===");
    debug!("üìù Updating Discord message to show AI processing...");
    trace!("üîç Discord message update: changing content to 'ü§ñ Generating summary...', command_uuid={}", command_uuid);
    response_msg.edit(ctx, |m| {
        m.content("ü§ñ Generating summary...")
    }).await?;
    debug!("‚úÖ Discord message updated to show AI processing");
    trace!("üîç Discord message update completed: command_uuid={}", command_uuid);
    
    // Stream the summary
    info!("üß† === AI SUMMARIZATION PHASE ===");
    info!("üß† Starting AI summarization process with streaming...");
    debug!("üöÄ AI summarization phase initiated");
    
    let content_length = if is_youtube {
        // For YouTube videos, calculate length from subtitle file
        if let Some(ref path) = subtitle_file_path {
            debug!("üìè === CONTENT LENGTH CALCULATION ===");
            debug!("üìè Calculating content length from subtitle file...");
            match fs::read_to_string(path) {
                Ok(content) => {
                    let cleaned_length = clean_vtt_content(&content).len();
                    debug!("üìè Content length from subtitle file: {} characters", cleaned_length);
                    trace!("üîç Content length calculation: path={}, length={}, command_uuid={}", path, cleaned_length, command_uuid);
                    cleaned_length
                },
                Err(e) => {
                    warn!("‚ö†Ô∏è Could not read subtitle file for length calculation: {}", e);
                    debug!("üîç Content length calculation error: path={}, error={}", path, e);
                    trace!("üîç Content length calculation error: path={}, error_type={}, command_uuid={}", 
                           path, std::any::type_name_of_val(&e), command_uuid);
                    0
                }
            }
        } else {
            0
        }
    } else {
        // For webpages, calculate length from content
        debug!("üìè Content length from direct content: {} characters", content.len());
        trace!("üîç Content length calculation: direct_length={}, command_uuid={}", content.len(), command_uuid);
        content.len()
    };
    
    trace!("üîç AI summarization phase: content_length={}, url={}, is_youtube={}, command_uuid={}", 
           content_length, url, is_youtube, command_uuid);
    let processing_start = std::time::Instant::now();
    debug!("‚è±Ô∏è AI processing start time: {:?}", processing_start);
    
    // For YouTube videos, pass empty content since RAG will handle the file processing
    // For webpages, pass the content directly
    let content_for_summary = if is_youtube { 
        debug!("üîß YouTube video detected - passing empty content for RAG processing");
        debug!("üîß File path for RAG: {:?}", subtitle_file_path);
        debug!("üîß File path exists: {}", subtitle_file_path.as_ref().map(|p| std::path::Path::new(p).exists()).unwrap_or(false));
        
        // Verify the subtitle file exists before proceeding
        if let Some(ref path) = subtitle_file_path {
            if !std::path::Path::new(path).exists() {
                error!("‚ùå === SUBTITLE FILE MISSING ERROR ===");
                error!("‚ùå Subtitle file does not exist: {}", path);
                response_msg.edit(ctx, |m| {
                    m.content(format!("‚ùå Subtitle file missing: {}", path))
                }).await?;
                return Ok(()); // Exit early if subtitle file is missing
            }
        } else {
            error!("‚ùå === NO SUBTITLE FILE PATH ERROR ===");
            error!("‚ùå No subtitle file path provided for YouTube video");
            response_msg.edit(ctx, |m| {
                m.content("‚ùå No subtitle file path provided for YouTube video")
            }).await?;
            return Ok(()); // Exit early if no subtitle file path
        }
        
        "" 
    } else { 
        debug!("üîß Webpage detected - passing content directly");
        &content 
    };
    match stream_summary(content_for_summary, url, &config, selected_model, &mut response_msg, ctx, is_youtube, subtitle_file_path.as_deref()).await {
        Ok(_) => {
            let processing_time = processing_start.elapsed();
            info!("‚úÖ === AI SUMMARIZATION SUCCESS ===");
            info!("‚úÖ Summary streaming completed successfully in {:.2}s", processing_time.as_secs_f64());
            debug!("üìä AI processing statistics: {:.2}s processing time", processing_time.as_secs_f64());
            debug!("üìä Processing time in milliseconds: {} ms", processing_time.as_millis());
            trace!("üîç AI summarization success: processing_time_ms={}, content_length={}, command_uuid={}", 
                   processing_time.as_millis(), content_length, command_uuid);
        },
        Err(e) => {
            error!("‚ùå === AI SUMMARIZATION ERROR ===");
            error!("‚ùå Summary generation failed: {}", e);
            debug!("üîç AI summarization error details: {:?}", e);
            debug!("üîç AI summarization error type: {:?}", std::any::type_name_of_val(&e));
            trace!("üîç AI summarization error: error_type={}, command_uuid={}", 
                   std::any::type_name_of_val(&e), command_uuid);
            response_msg.edit(ctx, |m| {
                m.content(format!("‚ùå Failed to generate summary: {}", e))
            }).await?;
            debug!("‚úÖ AI summarization error message sent to Discord");
        }
    }
    
    let total_time = start_time.elapsed();
    info!("‚è±Ô∏è === COMMAND COMPLETION ===");
    info!("‚è±Ô∏è Sum command completed in {:.2}s for user {} ({})", 
          total_time.as_secs_f64(), msg.author.name, msg.author.id);
    debug!("üìä === FINAL COMMAND STATISTICS ===");
    debug!("üìä Total execution time: {:.2}s", total_time.as_secs_f64());
    debug!("üìä Total execution time in milliseconds: {} ms", total_time.as_millis());
    debug!("üìä Content length: {} characters", content_length);
    debug!("üìä URL type: {}", if is_youtube { "YouTube" } else { "Webpage" });
    debug!("üìä User: {} ({})", msg.author.name, msg.author.id);
    debug!("üìä Channel: {} ({})", msg.channel_id, msg.channel_id.0);
    debug!("üìä Command UUID: {}", command_uuid);
    trace!("üîç Final command trace: total_time_ms={}, content_length={}, url_type={}, user_id={}, channel_id={}, command_uuid={}", 
           total_time.as_millis(), content_length, if is_youtube { "youtube" } else { "webpage" }, 
           msg.author.id, msg.channel_id, command_uuid);
    
    // Final comprehensive logging summary
    log::info!("üéØ === SUM COMMAND COMPLETION SUMMARY ===");
    log::info!("üéØ Command UUID: {}", command_uuid);
    log::info!("üéØ Total execution time: {:.2}s", total_time.as_secs_f64());
    log::info!("üéØ Content type: {}", if is_youtube { "YouTube" } else { "Webpage" });
    log::info!("üéØ Content length: {} characters", content_length);
    log::info!("üéØ User: {} ({})", msg.author.name, msg.author.id);
    log::info!("üéØ Channel: {} ({})", msg.channel_id, msg.channel_id.0);
    log::info!("üéØ URL: {}", url);
    log::info!("üéØ Processing method: {}", if subtitle_file_path.is_some() { "RAG with file" } else { "Direct processing" });
    
    // TEMPORARILY BYPASS CLEANUP - Keep temporary files for debugging
    if let Some(ref file_path) = subtitle_file_path {
        // Log file path
        log::info!("üéØ File path used: {}", file_path);
        log::info!("üîÑ === CLEANUP BYPASSED ===");
        log::info!("üîÑ Temporary file preserved for debugging: {}", file_path);
        log::info!("üîÑ File exists: {}", std::path::Path::new(&file_path).exists());
        log::info!("üîÑ Command UUID: {}", command_uuid);
        log::info!("üîÑ Status: Temporary file preserved (cleanup bypassed)");
        
        debug!("üîÑ === CLEANUP BYPASSED ===");
        debug!("üîÑ Preserving temporary file for debugging: {}", file_path);
        trace!("üîç Cleanup bypassed: path={}, command_uuid={}", file_path, command_uuid);
    }
    
    log::info!("üéØ Status: SUCCESS");
    
    // Trace-level function exit
    trace!("[TRACE][SUM] === FUNCTION EXIT: sum() ===");
    trace!("[TRACE][SUM] Function: sum(), UUID: {}", command_uuid);
    trace!("[TRACE][SUM] Exit status: SUCCESS");
    trace!("[TRACE][SUM] Total execution time: {:.3}s", total_time.as_secs_f64());
    trace!("[TRACE][SUM] Final content length: {} chars", content_length);
    trace!("[TRACE][SUM] Processing method used: {}", 
           if let Some(ref _path) = subtitle_file_path { "RAG with file" } else { "Direct processing" });
    trace!("[TRACE][SUM] Exit timestamp: {:?}", std::time::Instant::now());

    
    Ok(())
}

// Load summarization system prompt with multi-path fallback (like lm command)
// Loads summarization_prompt.txt from multiple locations, returns prompt string or fallback
async fn load_summarization_prompt() -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
    let prompt_paths = [
        "summarization_prompt.txt",
        "../summarization_prompt.txt",
        "../../summarization_prompt.txt",
        "src/summarization_prompt.txt",
        "example_summarization_prompt.txt",
        "../example_summarization_prompt.txt",
        "../../example_summarization_prompt.txt",
        "src/example_summarization_prompt.txt",
    ];
    
    for path in &prompt_paths {
        match fs::read_to_string(path) {
            Ok(content) => {
                // Remove BOM if present
                let content = content.strip_prefix('\u{feff}').unwrap_or(&content);
                debug!("üìÑ Summarization prompt loaded from: {}", path);
                return Ok(content.trim().to_string());
            }
            Err(_) => continue,
        }
    }
    
    // Fallback prompt if no file found
    debug!("üìÑ Using built-in fallback summarization prompt");
    Ok("You are an expert content summarizer. Create a comprehensive, well-structured summary of the provided content. Use clear formatting and highlight key points. Keep the summary informative but concise.".to_string())
}

// Load YouTube summarization system prompt with multi-path fallback
// Loads youtube_summarization_prompt.txt from multiple locations, returns prompt string or fallback
async fn load_youtube_summarization_prompt() -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
    let prompt_paths = [
        "youtube_summarization_prompt.txt",
        "../youtube_summarization_prompt.txt",
        "../../youtube_summarization_prompt.txt",
        "src/youtube_summarization_prompt.txt",
        "example_youtube_summarization_prompt.txt",
        "../example_youtube_summarization_prompt.txt",
        "../../example_youtube_summarization_prompt.txt",
        "src/example_youtube_summarization_prompt.txt",
    ];
    
    for path in &prompt_paths {
        match fs::read_to_string(path) {
            Ok(content) => {
                // Remove BOM if present
                let content = content.strip_prefix('\u{feff}').unwrap_or(&content);
                debug!("üì∫ YouTube summarization prompt loaded from: {}", path);
                return Ok(content.trim().to_string());
            }
            Err(_) => continue,
        }
    }
    
    // Fallback prompt if no file found
    debug!("üì∫ Using built-in fallback YouTube summarization prompt");
    Ok("You are an expert at summarizing YouTube video content. Focus on key points, main themes, and important takeaways. Structure your summary with clear sections and highlight the most valuable information for viewers.".to_string())
}

// Enhanced YouTube transcript fetcher using yt-dlp with detailed logging
// Generate a hash from YouTube URL for caching
fn generate_youtube_cache_key(url: &str) -> String {
    let mut hasher = Sha256::new();
    hasher.update(url.as_bytes());
    let result = hasher.finalize();
    format!("{:x}", result)
}

// Downloads and cleans VTT subtitles for a given YouTube URL with caching
async fn fetch_youtube_transcript(url: &str) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
    let process_uuid = Uuid::new_v4();
    
    // Trace-level function entry
    trace!("[TRACE][SUM][fetch_youtube_transcript] === FUNCTION ENTRY ===");
    trace!("[TRACE][SUM][fetch_youtube_transcript] Function: fetch_youtube_transcript()");
    trace!("[TRACE][SUM][fetch_youtube_transcript] Process UUID: {}", process_uuid);
    trace!("[TRACE][SUM][fetch_youtube_transcript] Input URL: '{}'", url);
    trace!("[TRACE][SUM][fetch_youtube_transcript] URL length: {} chars", url.len());
    trace!("[TRACE][SUM][fetch_youtube_transcript] Current working dir: {:?}", std::env::current_dir());
    
    info!("üé• === YOUTUBE TRANSCRIPT EXTRACTION STARTED ===");
    info!("üÜî Process UUID: {}", process_uuid);
    info!("üìç Target URL: {}", url);
    
    // TEMPORARILY BYPASS CACHING - Use direct yt_transcript files
    info!("üîÑ === CACHING BYPASSED ===");
    info!("üîÑ Using direct yt_transcript files for RAG processing");
    debug!("üîÑ Cache system temporarily disabled");
    trace!("üîç Cache bypass: process_uuid={}", process_uuid);
    
    let temp_file = format!("yt_transcript_{}", Uuid::new_v4());
    info!("üìÅ Temp file base: {}", temp_file);
    
    debug!("üîß === YOUTUBE TRANSCRIPT INITIALIZATION ===");
    debug!("üîß URL length: {} characters", url.len());
    debug!("üîß Temp file length: {} characters", temp_file.len());
    debug!("üîß Process UUID: {}", process_uuid);
    trace!("üîç YouTube transcript extraction details: url_length={}, temp_file_length={}, uuid={}", 
           url.len(), temp_file.len(), process_uuid);
    
    // Create subtitles directory if it doesn't exist
    debug!("üìÅ === DIRECTORY SETUP ===");
    let subtitles_dir = "subtitles";
    debug!("üìÅ Checking subtitles directory: {}", subtitles_dir);
    debug!("üìÅ Directory exists: {}", std::path::Path::new(subtitles_dir).exists());
    trace!("üîç Directory check: path={}, exists={}, process_uuid={}", subtitles_dir, std::path::Path::new(subtitles_dir).exists(), process_uuid);
    
    if !std::path::Path::new(subtitles_dir).exists() {
        debug!("üìÅ Creating subtitles directory: {}", subtitles_dir);
        trace!("üîç Directory creation started: path={}, process_uuid={}", subtitles_dir, process_uuid);
        std::fs::create_dir(subtitles_dir)?;
        debug!("‚úÖ Subtitles directory created successfully");
        trace!("üîç Directory creation completed: path={}, process_uuid={}", subtitles_dir, process_uuid);
    } else {
        debug!("üìÅ Subtitles directory already exists: {}", subtitles_dir);
        trace!("üîç Directory already exists: path={}, process_uuid={}", subtitles_dir, process_uuid);
    }
    
    // Check if yt-dlp is available and get version
    debug!("üîç === YT-DLP VERSION CHECK ===");
    debug!("üîç Checking yt-dlp availability and version...");
    trace!("üîç yt-dlp version check started: process_uuid={}", process_uuid);
    
    let version_output = Command::new("yt-dlp")
        .arg("--version")
        .output()
        .map_err(|e| {
            error!("‚ùå === YT-DLP NOT FOUND ERROR ===");
            error!("‚ùå yt-dlp is not installed or not in PATH: {}", e);
            debug!("üîç yt-dlp PATH error details: {:?}", e);
            debug!("üîç yt-dlp PATH error type: {:?}", std::any::type_name_of_val(&e));
            trace!("üîç yt-dlp PATH error: error_type={}, process_uuid={}", 
                   std::any::type_name_of_val(&e), process_uuid);
            "yt-dlp is not installed. Please install yt-dlp to use YouTube summarization."
        })?;
    
    debug!("üìä === YT-DLP VERSION CHECK RESULTS ===");
    debug!("üìä yt-dlp version check exit status: {}", version_output.status);
    debug!("üìä yt-dlp version check success: {}", version_output.status.success());
    debug!("üìä yt-dlp stdout length: {} bytes", version_output.stdout.len());
    debug!("üìä yt-dlp stderr length: {} bytes", version_output.stderr.len());
    trace!("üîç yt-dlp version check details: success={}, stdout_len={}, stderr_len={}, process_uuid={}", 
           version_output.status.success(), version_output.stdout.len(), version_output.stderr.len(), process_uuid);
    
    if !version_output.status.success() {
        error!("‚ùå === YT-DLP VERSION CHECK FAILED ===");
        error!("‚ùå yt-dlp version check failed");
        debug!("üîç yt-dlp version check stderr: {}", String::from_utf8_lossy(&version_output.stderr));
        debug!("üîç yt-dlp version check exit code: {:?}", version_output.status.code());
        trace!("üîç yt-dlp version check failure: exit_code={:?}, process_uuid={}", version_output.status.code(), process_uuid);
        return Err("yt-dlp is not working properly".into());
    }
    
    let version_str = String::from_utf8_lossy(&version_output.stdout);
    info!("‚úÖ === YT-DLP VERSION CHECK SUCCESS ===");
    info!("‚úÖ yt-dlp version: {}", version_str.trim());
    debug!("üîß yt-dlp version check completed successfully");
    debug!("üîß Version string length: {} characters", version_str.trim().len());
    trace!("üîç yt-dlp version check success: version={}, version_length={}, process_uuid={}", 
           version_str.trim(), version_str.trim().len(), process_uuid);
    
    // Try multiple subtitle extraction methods with retry logic
    info!("üîÑ === SUBTITLE EXTRACTION PHASE ===");
    debug!("üîÑ Starting subtitle extraction with retry logic...");
    trace!("üîç Subtitle extraction phase started: process_uuid={}", process_uuid);
    
    let mut success = false;
    let mut last_error = String::new();
    let max_retries = 3;
    
    debug!("üìä === EXTRACTION CONFIGURATION ===");
    debug!("üìä Max retries: {}", max_retries);
    debug!("üìä Sleep interval: 2 seconds");
    debug!("üìä Max sleep interval: 5 seconds");
    debug!("üìä Temp file: {}", temp_file);
    debug!("üìä Subtitles directory: {}", subtitles_dir);
    trace!("üîç Extraction configuration details: max_retries={}, temp_file={}, subtitles_dir={}, process_uuid={}", 
           max_retries, temp_file, subtitles_dir, process_uuid);
    
    for attempt in 1..=max_retries {
        info!("üîÑ === ATTEMPT {}/{} STARTED ===", attempt, max_retries);
        debug!("üîÑ Attempt {} of {} started", attempt, max_retries);
        trace!("üîç Attempt {} started: attempt_number={}, max_retries={}, process_uuid={}", 
               attempt, attempt, max_retries, process_uuid);
        
        // Method 1: Try automatic subtitles first
        debug!("üîÑ === METHOD 1: AUTOMATIC SUBTITLES ===");
        debug!("üîÑ Method 1: Trying automatic subtitles...");
        trace!("üîç Method 1 (automatic subtitles) started: attempt={}, process_uuid={}", attempt, process_uuid);
        
        let mut command = Command::new("yt-dlp");
        command
            .arg("--write-auto-sub")
            .arg("--write-sub")
            .arg("--sub-langs").arg("en")
            .arg("--sub-format").arg("vtt")
            .arg("--skip-download")
            .arg("--no-warnings")
            .arg("--no-playlist")
            .arg("--sleep-interval").arg("2")  // Add 2 second delay between requests
            .arg("--max-sleep-interval").arg("5")  // Max 5 second delay
            .arg("--retries").arg("3")  // Retry failed downloads
            .arg("--fragment-retries").arg("3")  // Retry failed fragments
            .arg("--user-agent").arg("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")  // Use realistic user agent
            .arg("--output").arg(&format!("{}/{}", subtitles_dir, temp_file))
            .arg(url);
        
        debug!("üìã === YT-DLP COMMAND ARGUMENTS ===");
        debug!("üìã yt-dlp command arguments:");
        debug!("üìã   - --write-auto-sub");
        debug!("üìã   - --write-sub");
        debug!("üìã   - --sub-langs en");
        debug!("üìã   - --sub-format vtt");
        debug!("üìã   - --skip-download");
        debug!("üìã   - --no-warnings");
        debug!("üìã   - --no-playlist");
        debug!("üìã   - --sleep-interval 2");
        debug!("üìã   - --max-sleep-interval 5");
        debug!("üìã   - --retries 3");
        debug!("üìã   - --fragment-retries 3");
        debug!("üìã   - --user-agent [Chrome 120]");
        debug!("üìã   - --output {}/{}", subtitles_dir, temp_file);
        debug!("üìã   - URL: {}", url);
        trace!("üîç yt-dlp command details: attempt={}, output_path={}/{}, url_length={}, process_uuid={}", 
               attempt, subtitles_dir, temp_file, url.len(), process_uuid);
        
        debug!("üöÄ === YT-DLP COMMAND EXECUTION ===");
        debug!("üöÄ Executing yt-dlp command...");
        trace!("üîç yt-dlp command execution started: attempt={}, process_uuid={}", attempt, process_uuid);
        
        let output = command.output()?;
        
        debug!("üìä === YT-DLP COMMAND RESULTS ===");
        debug!("üìä yt-dlp command completed with exit status: {}", output.status);
        debug!("üìä yt-dlp command success: {}", output.status.success());
        debug!("üìä yt-dlp stdout length: {} bytes", output.stdout.len());
        debug!("üìä yt-dlp stderr length: {} bytes", output.stderr.len());
        trace!("üîç yt-dlp command execution completed: success={}, stdout_len={}, stderr_len={}, attempt={}, process_uuid={}", 
               output.status.success(), output.stdout.len(), output.stderr.len(), attempt, process_uuid);
        
        if output.status.success() {
            success = true;
            info!("‚úÖ === METHOD 1 SUCCESS ===");
            info!("‚úÖ Method 1 (automatic subtitles) succeeded on attempt {}", attempt);
            debug!("üìÑ yt-dlp stdout: {}", String::from_utf8_lossy(&output.stdout));
            trace!("üîç Method 1 success details: attempt={}, stdout_length={}, process_uuid={}", 
                   attempt, output.stdout.len(), process_uuid);
            break;
        } else {
            let stderr = String::from_utf8_lossy(&output.stderr);
            let stdout = String::from_utf8_lossy(&output.stdout);
            last_error = stderr.to_string();
            
            warn!("‚ùå === METHOD 1 FAILED ===");
            warn!("‚ùå Method 1 failed on attempt {}", attempt);
            debug!("üìÑ yt-dlp stdout: {}", stdout);
            debug!("‚ùå yt-dlp stderr: {}", stderr);
            debug!("‚ùå stderr length: {} characters", stderr.len());
            debug!("‚ùå stdout length: {} characters", stdout.len());
            trace!("üîç Method 1 failure details: attempt={}, stderr_length={}, stdout_length={}, process_uuid={}", 
                   attempt, stderr.len(), stdout.len(), process_uuid);
            
            // Check if it's a rate limit error
            debug!("üîç === RATE LIMIT CHECK ===");
            debug!("üîç Checking for rate limit errors...");
            debug!("üîç stderr contains '429': {}", stderr.contains("429"));
            debug!("üîç stderr contains 'Too Many Requests': {}", stderr.contains("Too Many Requests"));
            trace!("üîç Rate limit detection: stderr_contains_429={}, stderr_contains_too_many_requests={}, attempt={}, process_uuid={}", 
                   stderr.contains("429"), stderr.contains("Too Many Requests"), attempt, process_uuid);
            
            if stderr.contains("429") || stderr.contains("Too Many Requests") {
                warn!("üö® === RATE LIMIT DETECTED ===");
                warn!("üö® Rate limit detected (429/Too Many Requests)");
                trace!("üîç Rate limit detection: stderr_contains_429={}, stderr_contains_too_many_requests={}, attempt={}, process_uuid={}", 
                       stderr.contains("429"), stderr.contains("Too Many Requests"), attempt, process_uuid);
                
                if attempt < max_retries {
                    let delay = attempt * 5; // Exponential backoff: 5s, 10s, 15s
                    warn!("‚è≥ === RATE LIMIT DELAY ===");
                    warn!("‚è≥ Rate limited. Waiting {} seconds before retry...", delay);
                    debug!("‚è≥ Delay calculation: attempt={}, delay_seconds={}", attempt, delay);
                    trace!("üîç Rate limit delay: delay_seconds={}, attempt={}, max_retries={}, process_uuid={}", 
                           delay, attempt, max_retries, process_uuid);
                    
                    tokio::time::sleep(tokio::time::Duration::from_secs(delay)).await;
                    debug!("‚úÖ Wait completed, proceeding to retry");
                    trace!("üîç Rate limit delay completed, continuing to next attempt: process_uuid={}", process_uuid);
                    continue;
                } else {
                    warn!("‚ùå === MAX RETRIES REACHED ===");
                    warn!("‚ùå Max retries reached, cannot retry rate limit");
                    debug!("‚ùå Final attempt reached: attempt={}, max_retries={}", attempt, max_retries);
                    trace!("üîç Max retries reached: attempt={}, max_retries={}, process_uuid={}", attempt, max_retries, process_uuid);
                }
            }
            
            // Method 2: Try manual subtitles only
            debug!("üîÑ === METHOD 2: MANUAL SUBTITLES ===");
            debug!("üîÑ Method 2: Trying manual subtitles only...");
            trace!("üîç Method 2 (manual subtitles) started: attempt={}, process_uuid={}", attempt, process_uuid);
            
            let mut command2 = Command::new("yt-dlp");
            command2
                .arg("--write-sub")
                .arg("--sub-langs").arg("en")
                .arg("--sub-format").arg("vtt")
                .arg("--skip-download")
                .arg("--no-warnings")
                .arg("--no-playlist")
                .arg("--sleep-interval").arg("2")  // Add 2 second delay between requests
                .arg("--max-sleep-interval").arg("5")  // Max 5 second delay
                .arg("--retries").arg("3")  // Retry failed downloads
                .arg("--fragment-retries").arg("3")  // Retry failed fragments
                .arg("--user-agent").arg("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")  // Use realistic user agent
                .arg("--output").arg(&format!("{}/{}", subtitles_dir, temp_file))
                .arg(url);
            
            debug!("üìã === METHOD 2 COMMAND ARGUMENTS ===");
            debug!("üìã Method 2 yt-dlp command arguments:");
            debug!("üìã   - --write-sub");
            debug!("üìã   - --sub-langs en");
            debug!("üìã   - --sub-format vtt");
            debug!("üìã   - --skip-download");
            debug!("üìã   - --no-warnings");
            debug!("üìã   - --no-playlist");
            debug!("üìã   - --sleep-interval 2");
            debug!("üìã   - --max-sleep-interval 5");
            debug!("üìã   - --retries 3");
            debug!("üìã   - --fragment-retries 3");
            debug!("üìã   - --user-agent [Chrome 120]");
            debug!("üìã   - --output {}/{}", subtitles_dir, temp_file);
            debug!("üìã   - URL: {}", url);
            trace!("üîç Method 2 command details: attempt={}, output_path={}/{}, url_length={}, process_uuid={}", 
                   attempt, subtitles_dir, temp_file, url.len(), process_uuid);
            
            debug!("üöÄ === METHOD 2 COMMAND EXECUTION ===");
            debug!("üöÄ Executing Method 2 yt-dlp command...");
            trace!("üîç Method 2 command execution started: attempt={}, process_uuid={}", attempt, process_uuid);
            
            let output2 = command2.output()?;
            
            debug!("üìä === METHOD 2 COMMAND RESULTS ===");
            debug!("üìä Method 2 yt-dlp command completed with exit status: {}", output2.status);
            debug!("üìä Method 2 yt-dlp command success: {}", output2.status.success());
            debug!("üìä Method 2 yt-dlp stdout length: {} bytes", output2.stdout.len());
            debug!("üìä Method 2 yt-dlp stderr length: {} bytes", output2.stderr.len());
            trace!("üîç Method 2 command execution completed: success={}, stdout_len={}, stderr_len={}, attempt={}, process_uuid={}", 
                   output2.status.success(), output2.stdout.len(), output2.stderr.len(), attempt, process_uuid);
            
            if output2.status.success() {
                success = true;
                info!("‚úÖ === METHOD 2 SUCCESS ===");
                info!("‚úÖ Method 2 (manual subtitles) succeeded on attempt {}", attempt);
                debug!("üìÑ Method 2 yt-dlp stdout: {}", String::from_utf8_lossy(&output2.stdout));
                trace!("üîç Method 2 success details: attempt={}, stdout_length={}, process_uuid={}", 
                       attempt, output2.stdout.len(), process_uuid);
                break;
            } else {
                let stderr2 = String::from_utf8_lossy(&output2.stderr);
                last_error = stderr2.to_string();
                
                warn!("‚ùå === METHOD 2 FAILED ===");
                warn!("‚ùå Method 2 failed on attempt {}: {}", attempt, stderr2);
                debug!("üìÑ Method 2 yt-dlp stdout: {}", String::from_utf8_lossy(&output2.stdout));
                debug!("‚ùå Method 2 yt-dlp stderr: {}", stderr2);
                debug!("‚ùå Method 2 stderr length: {} characters", stderr2.len());
                debug!("‚ùå Method 2 stdout length: {} characters", output2.stdout.len());
                trace!("üîç Method 2 failure details: attempt={}, stderr_length={}, stdout_length={}, process_uuid={}", 
                       attempt, stderr2.len(), output2.stdout.len(), process_uuid);
                
                // Check if it's a rate limit error
                debug!("üîç === METHOD 2 RATE LIMIT CHECK ===");
                debug!("üîç Checking Method 2 for rate limit errors...");
                debug!("üîç Method 2 stderr contains '429': {}", stderr2.contains("429"));
                debug!("üîç Method 2 stderr contains 'Too Many Requests': {}", stderr2.contains("Too Many Requests"));
                trace!("üîç Method 2 rate limit detection: stderr_contains_429={}, stderr_contains_too_many_requests={}, attempt={}, process_uuid={}", 
                       stderr2.contains("429"), stderr2.contains("Too Many Requests"), attempt, process_uuid);
                
                if stderr2.contains("429") || stderr2.contains("Too Many Requests") {
                    if attempt < max_retries {
                        let delay = attempt * 5; // Exponential backoff: 5s, 10s, 15s
                        warn!("‚è≥ === METHOD 2 RATE LIMIT DELAY ===");
                        warn!("‚è≥ Rate limited. Waiting {} seconds before retry...", delay);
                        debug!("‚è≥ Method 2 delay calculation: attempt={}, delay_seconds={}", attempt, delay);
                        trace!("üîç Method 2 rate limit delay: delay_seconds={}, attempt={}, max_retries={}, process_uuid={}", 
                               delay, attempt, max_retries, process_uuid);
                        
                        tokio::time::sleep(tokio::time::Duration::from_secs(delay)).await;
                        debug!("‚úÖ Method 2 wait completed, proceeding to retry");
                        trace!("üîç Method 2 rate limit delay completed, continuing to next attempt: process_uuid={}", process_uuid);
                        continue;
                    }
                }
            }
        }
    }
    
    if !success {
        error!("‚ùå === ALL SUBTITLE EXTRACTION METHODS FAILED ===");
        error!("‚ùå All subtitle extraction methods failed");
        error!("‚ùå Last error: {}", last_error);
        debug!("üîç Final failure summary: success={}, last_error_length={}, process_uuid={}", 
               success, last_error.len(), process_uuid);
        trace!("üîç All methods failed: success={}, last_error={}, process_uuid={}", 
               success, last_error, process_uuid);
        
        // Check for common error patterns and provide helpful messages
        debug!("üîç === ERROR PATTERN ANALYSIS ===");
        debug!("üîç Analyzing error patterns for helpful messages...");
        debug!("üîç Error contains 'Did not get any data blocks': {}", last_error.contains("Did not get any data blocks"));
        debug!("üîç Error contains 'Sign in to confirm you're not a bot': {}", last_error.contains("Sign in to confirm you're not a bot"));
        debug!("üîç Error contains 'Private video': {}", last_error.contains("Private video"));
        debug!("üîç Error contains 'Video unavailable': {}", last_error.contains("Video unavailable"));
        debug!("üîç Error contains '429': {}", last_error.contains("429"));
        debug!("üîç Error contains 'Too Many Requests': {}", last_error.contains("Too Many Requests"));
        debug!("üîç Error contains 'No subtitles': {}", last_error.contains("No subtitles"));
        debug!("üîç Error contains 'no automatic captions': {}", last_error.contains("no automatic captions"));
        debug!("üîç Error contains 'This video is not available': {}", last_error.contains("This video is not available"));
        debug!("üîç Error contains '403': {}", last_error.contains("403"));
        debug!("üîç Error contains 'Forbidden': {}", last_error.contains("Forbidden"));
        debug!("üîç Error contains 'fragment 1 not found': {}", last_error.contains("fragment 1 not found"));
        trace!("üîç Error pattern analysis: data_blocks={}, bot_confirmation={}, private_video={}, video_unavailable={}, rate_limit={}, forbidden={}, fragment_not_found={}, no_subtitles={}, not_available={}, process_uuid={}", 
               last_error.contains("Did not get any data blocks"), last_error.contains("Sign in to confirm you're not a bot"), 
               last_error.contains("Private video"), last_error.contains("Video unavailable"), 
               last_error.contains("429") || last_error.contains("Too Many Requests"),
               last_error.contains("403") || last_error.contains("Forbidden"),
               last_error.contains("fragment 1 not found"),
               last_error.contains("No subtitles") || last_error.contains("no automatic captions"),
               last_error.contains("This video is not available"), process_uuid);
        
        if last_error.contains("Did not get any data blocks") {
            return Err("YouTube subtitles extraction failed: 'Did not get any data blocks'. This is usually caused by YouTube's anti-bot measures or an outdated yt-dlp version. Try updating yt-dlp with: yt-dlp -U".into());
        }
        
        if last_error.contains("Sign in to confirm you're not a bot") {
            return Err("YouTube is blocking requests: 'Sign in to confirm you're not a bot'. This is a temporary YouTube restriction. Try again later or use a different video.".into());
        }
        
        if last_error.contains("Private video") || last_error.contains("Video unavailable") {
            return Err("Video is private or unavailable. Please check the URL and try again.".into());
        }
        
        if last_error.contains("429") || last_error.contains("Too Many Requests") {
            return Err("YouTube is rate limiting requests. Please wait a few minutes and try again, or try a different video.".into());
        }
        
        if last_error.contains("403") || last_error.contains("Forbidden") {
            return Err("YouTube is blocking access to this video (403 Forbidden). This could be due to:\n‚Ä¢ Video is age-restricted or private\n‚Ä¢ YouTube's anti-bot measures\n‚Ä¢ Regional restrictions\n‚Ä¢ Try updating yt-dlp: `yt-dlp -U`\n‚Ä¢ Try a different video or wait and retry later.".into());
        }
        
        if last_error.contains("fragment 1 not found") {
            return Err("YouTube video fragment not found. This usually indicates:\n‚Ä¢ Video is being processed or temporarily unavailable\n‚Ä¢ YouTube's servers are having issues\n‚Ä¢ Try again in a few minutes\n‚Ä¢ If persistent, try updating yt-dlp: `yt-dlp -U`".into());
        }
        
        if last_error.contains("No subtitles") || last_error.contains("no automatic captions") {
            return Err("This video has no automatic captions or subtitles available.".into());
        }
        
        if last_error.contains("Video unavailable") {
            return Err("This video is unavailable or has been removed.".into());
        }
        
        if last_error.contains("This video is not available") {
            return Err("This video is not available in your region or has been made private.".into());
        }
        
        return Err(format!("yt-dlp failed to extract subtitles: {}", last_error).into());
    }
    
    info!("‚úÖ === YT-DLP SUBTITLE EXTRACTION SUCCESS ===");
    info!("‚úÖ yt-dlp subtitle extraction completed successfully");
    debug!("üîß Subtitle extraction phase completed: success={}, process_uuid={}", success, process_uuid);
    trace!("üîç yt-dlp subtitle extraction success: process_uuid={}", process_uuid);
    
    // Look for the subtitle file with multiple possible naming patterns
    debug!("üìÑ === SUBTITLE FILE SEARCH ===");
    debug!("üìÑ Looking for subtitle files with multiple naming patterns...");
    
    let possible_vtt_files = vec![
        format!("{}/{}.en.vtt", subtitles_dir, temp_file),
        format!("{}/{}.en-auto.vtt", subtitles_dir, temp_file),
        format!("{}/{}.en-manual.vtt", subtitles_dir, temp_file),
        format!("{}/{}.vtt", subtitles_dir, temp_file),
    ];
    
    debug!("üìÑ Possible VTT file patterns: {:?}", possible_vtt_files);
    trace!("üîç Subtitle file search: patterns={:?}, process_uuid={}", possible_vtt_files, process_uuid);
    
    // List all files in the subtitles directory that match the temp_file pattern
    debug!("üìÅ === DIRECTORY SCAN ===");
    debug!("üìÅ Scanning subtitles directory for matching files...");
    if let Ok(entries) = std::fs::read_dir(subtitles_dir) {
        let files: Vec<String> = entries
            .filter_map(|e| e.ok())
            .map(|e| e.file_name().to_string_lossy().to_string())
            .filter(|name| name.contains(&temp_file) && name.ends_with(".vtt"))
            .collect();
        debug!("üìÅ Found VTT files in subtitles directory: {:?}", files);
        debug!("üìÅ Total matching files found: {}", files.len());
        trace!("üîç Directory scan: found_files={:?}, file_count={}, process_uuid={}", files, files.len(), process_uuid);
    } else {
        warn!("‚ö†Ô∏è Could not read subtitles directory for file listing");
        debug!("üîç Directory read error: path={}, process_uuid={}", subtitles_dir, process_uuid);
    }
    
    let mut vtt_file = None;
    debug!("üîç === FILE PATTERN MATCHING ===");
    for (i, file_path) in possible_vtt_files.iter().enumerate() {
        debug!("üîç Checking pattern {}: {}", i+1, file_path);
        debug!("üîç File exists: {}", std::path::Path::new(file_path).exists());
        trace!("üîç File check: pattern={}, exists={}, process_uuid={}", file_path, std::path::Path::new(file_path).exists(), process_uuid);
        
        if std::path::Path::new(file_path).exists() {
            vtt_file = Some(file_path.clone());
            info!("‚úÖ === SUBTITLE FILE FOUND ===");
            info!("‚úÖ Found subtitle file: {}", file_path);
            debug!("üìÑ Selected subtitle file: {}", file_path);
            debug!("üìÑ File size: {} bytes", std::fs::metadata(file_path).map(|m| m.len()).unwrap_or(0));
            trace!("üîç Subtitle file selected: path={}, size={}, process_uuid={}", 
                   file_path, std::fs::metadata(file_path).map(|m| m.len()).unwrap_or(0), process_uuid);
            break;
        } else {
            trace!("üîç Subtitle file not found: path={}, process_uuid={}", file_path, process_uuid);
        }
    }
    
    let vtt_file = match vtt_file {
        Some(path) => path,
        None => {
            error!("‚ùå === NO SUBTITLE FILE FOUND ===");
            error!("‚ùå No subtitle file found with any expected pattern");
            debug!("üîç File search failed: checked_patterns={}, process_uuid={}", possible_vtt_files.len(), process_uuid);
            
            // List files in subtitles directory for debugging
            debug!("üìÅ === DEBUGGING DIRECTORY CONTENTS ===");
            if let Ok(entries) = std::fs::read_dir(subtitles_dir) {
                let files: Vec<String> = entries
                    .filter_map(|e| e.ok())
                    .map(|e| e.file_name().to_string_lossy().to_string())
                    .filter(|name| name.contains(&temp_file) && name.ends_with(".vtt"))
                    .collect();
                debug!("üìÅ Found VTT files in subtitles directory: {:?}", files);
                debug!("üìÅ Total matching files found: {}", files.len());
                trace!("üîç Directory scan (on error): found_files={:?}, file_count={}, process_uuid={}", files, files.len(), process_uuid);
            }
            return Err("Subtitle file was not created by yt-dlp. The video may not have automatic captions available.".into());
        }
    };
    
    debug!("üìñ === SUBTITLE FILE READING ===");
    debug!("üìñ Reading subtitle file: {}", vtt_file);
    trace!("üîç Subtitle file read started: path={}, process_uuid={}", vtt_file, process_uuid);
    
    let content = fs::read_to_string(&vtt_file)?;
    
    debug!("üìñ === SUBTITLE FILE READ SUCCESS ===");
    debug!("üìñ Read subtitle file: {} characters from {}", content.len(), vtt_file);
    debug!("üìñ File content preview: {}", &content[..std::cmp::min(100, content.len())]);
    debug!("üìñ File content contains 'WEBVTT': {}", content.contains("WEBVTT"));
    debug!("üìñ File content is empty: {}", content.trim().is_empty());
    trace!("üîç Subtitle file read: path={}, length={}, preview='{}', process_uuid={}", 
           vtt_file, content.len(), &content[..std::cmp::min(100, content.len())], process_uuid);
    
    // Check if content is valid
    debug!("üîç === SUBTITLE CONTENT VALIDATION ===");
    debug!("üîç Validating subtitle file content...");
    
    if content.trim().is_empty() {
        error!("‚ùå === EMPTY SUBTITLE FILE ERROR ===");
        error!("‚ùå Downloaded subtitle file is empty: {}", vtt_file);
        debug!("üîç Subtitle file empty: path={}, content_length={}", vtt_file, content.len());
        trace!("üîç Subtitle file empty: path={}, process_uuid={}", vtt_file, process_uuid);
        return Err("Downloaded subtitle file is empty".into());
    }
    
    if !content.contains("WEBVTT") {
        error!("‚ùå === INVALID VTT FILE ERROR ===");
        error!("‚ùå Downloaded file is not a valid VTT subtitle file: {}", vtt_file);
        debug!("üîç Subtitle file missing WEBVTT header: path={}", vtt_file);
        debug!("üîç File content starts with: {}", &content[..std::cmp::min(50, content.len())]);
        trace!("üîç Subtitle file missing WEBVTT header: path={}, process_uuid={}", vtt_file, process_uuid);
        return Err("Downloaded file is not a valid VTT subtitle file".into());
    }
    
    debug!("‚úÖ Subtitle content validation passed");
    trace!("üîç Subtitle content validation success: path={}, process_uuid={}", vtt_file, process_uuid);
    
    // Clean VTT content
    debug!("üßπ === VTT CONTENT CLEANING ===");
    debug!("üßπ Cleaning VTT content from file: {}", vtt_file);
    trace!("üîç VTT cleaning started: original_length={}, process_uuid={}", content.len(), process_uuid);
    
    let cleaned = clean_vtt_content(&content);
    
    debug!("‚úÖ === VTT CLEANING COMPLETED ===");
    debug!("‚úÖ VTT content cleaned: {} characters", cleaned.len());
    debug!("‚úÖ Cleaning ratio: {:.2}%", (cleaned.len() as f64 / content.len() as f64) * 100.0);
    debug!("‚úÖ Cleaned content preview: {}", &cleaned[..std::cmp::min(100, cleaned.len())]);
    trace!("üîç VTT cleaning completed: cleaned_length={}, preview='{}', process_uuid={}", 
           cleaned.len(), &cleaned[..std::cmp::min(100, cleaned.len())], process_uuid);
    
    if cleaned.trim().is_empty() {
        error!("‚ùå === EMPTY CLEANED CONTENT ERROR ===");
        error!("‚ùå No readable text found in subtitle file after cleaning: {}", vtt_file);
        debug!("üîç Cleaned subtitle file empty: path={}, original_length={}, cleaned_length={}", vtt_file, content.len(), cleaned.len());
        trace!("üîç Cleaned subtitle file empty: path={}, process_uuid={}", vtt_file, process_uuid);
        return Err("No readable text found in subtitle file after cleaning".into());
    }
    
    info!("‚úÖ === YOUTUBE TRANSCRIPT EXTRACTION COMPLETED ===");
    info!("‚úÖ YouTube transcript extraction completed successfully");
    debug!("üìÑ Final subtitle file: {}", vtt_file);
    debug!("üìÑ Original content: {} characters", content.len());
    debug!("üìÑ Cleaned content: {} characters", cleaned.len());
    debug!("üìÑ Process UUID: {}", process_uuid);
    trace!("üîç YouTube transcript extraction success: file_path={}, original_length={}, cleaned_length={}, process_uuid={}", 
           vtt_file, content.len(), cleaned.len(), process_uuid);
    
    // TEMPORARILY BYPASS CACHING - Return temporary file directly
    info!("üîÑ === RETURNING TEMPORARY FILE PATH ===");
    info!("üîÑ Returning temporary file path for RAG processing: {}", vtt_file);
    debug!("üìÑ Temporary file path: {}", vtt_file);
    trace!("üîç Returning temporary path: temp={}, process_uuid={}", vtt_file, process_uuid);
    
    Ok(vtt_file)
}

// Enhanced VTT cleaner
// Removes timestamps, tags, and empty lines from VTT subtitle content
fn clean_vtt_content(vtt: &str) -> String {
    debug!("üßπ === VTT CLEANING STARTED ===");
    debug!("üßπ Cleaning VTT content...");
    debug!("üßπ Original VTT content length: {} characters", vtt.len());
    debug!("üßπ Original VTT line count: {}", vtt.lines().count());
    trace!("üîç VTT cleaning started: original_length={}, line_count={}", vtt.len(), vtt.lines().count());
    
    let mut lines = Vec::new();
    let mut processed_lines = 0;
    let mut skipped_lines = 0;
    let mut kept_lines = 0;
    
    debug!("üìù === LINE PROCESSING ===");
    for (line_num, line) in vtt.lines().enumerate() {
        processed_lines += 1;
        let original_line = line;
        let line = line.trim();
        
        // Skip headers, timestamps, and empty lines
        let is_empty = line.is_empty();
        let is_webvtt = line.starts_with("WEBVTT");
        let is_note = line.starts_with("NOTE");
        let is_timestamp = line.contains("-->");
        let is_numeric = line.chars().all(|c| c.is_numeric() || c == ':' || c == '.' || c == ' ');
        
        if is_empty || is_webvtt || is_note || is_timestamp || is_numeric {
            skipped_lines += 1;
            trace!("üîç Line {} skipped: empty={}, webvtt={}, note={}, timestamp={}, numeric={}, content='{}'", 
                   line_num + 1, is_empty, is_webvtt, is_note, is_timestamp, is_numeric, original_line);
            continue;
        }
        
        // Clean various subtitle tags
        debug!("üßπ === TAG CLEANING ===");
        let mut cleaned = line.to_string();
        
        // Track tag removals
        let original_cleaned = cleaned.clone();
        let mut tags_removed = 0;
        
        // Remove various subtitle tags
        if cleaned.contains("<c>") {
            cleaned = cleaned.replace("<c>", "");
            tags_removed += 1;
        }
        if cleaned.contains("</c>") {
            cleaned = cleaned.replace("</c>", "");
            tags_removed += 1;
        }
        if cleaned.contains("<v ") {
            cleaned = cleaned.replace("<v ", "");
            tags_removed += 1;
        }
        if cleaned.contains("</v>") {
            cleaned = cleaned.replace("</v>", "");
            tags_removed += 1;
        }
        if cleaned.contains("<b>") {
            cleaned = cleaned.replace("<b>", "");
            tags_removed += 1;
        }
        if cleaned.contains("</b>") {
            cleaned = cleaned.replace("</b>", "");
            tags_removed += 1;
        }
        if cleaned.contains("<i>") {
            cleaned = cleaned.replace("<i>", "");
            tags_removed += 1;
        }
        if cleaned.contains("</i>") {
            cleaned = cleaned.replace("</i>", "");
            tags_removed += 1;
        }
        if cleaned.contains("<u>") {
            cleaned = cleaned.replace("<u>", "");
            tags_removed += 1;
        }
        if cleaned.contains("</u>") {
            cleaned = cleaned.replace("</u>", "");
            tags_removed += 1;
        }
        
        cleaned = cleaned.trim().to_string();
        
        if tags_removed > 0 {
            trace!("üîç Line {} tag cleaning: removed {} tags, '{}' -> '{}'", 
                   line_num + 1, tags_removed, original_cleaned, cleaned);
        }
        
        if !cleaned.is_empty() {
            lines.push(cleaned.clone());
            kept_lines += 1;
            trace!("üîç Line {} kept: '{}'", line_num + 1, cleaned);
        } else {
            skipped_lines += 1;
            trace!("üîç Line {} skipped after cleaning: was '{}'", line_num + 1, original_line);
        }
    }
    
    debug!("üìä === LINE PROCESSING STATISTICS ===");
    debug!("üìä Total lines processed: {}", processed_lines);
    debug!("üìä Lines skipped: {}", skipped_lines);
    debug!("üìä Lines kept: {}", kept_lines);
    debug!("üìä Keep ratio: {:.2}%", (kept_lines as f64 / processed_lines as f64) * 100.0);
    
    let result = lines.join(" ");
    debug!("üîó === LINE JOINING ===");
    debug!("üîó Joined {} lines into single string", lines.len());
    debug!("üîó Result length: {} characters", result.len());
    trace!("üîç Line joining completed: line_count={}, result_length={}", lines.len(), result.len());
    
    // Additional cleanup: remove excessive whitespace
    debug!("üßπ === WHITESPACE CLEANUP ===");
    let _original_result = result.clone();
    let final_result = result
        .split_whitespace()
        .collect::<Vec<_>>()
        .join(" ");
    
    debug!("üßπ === FINAL VTT CLEANING COMPLETED ===");
    debug!("üßπ VTT cleaning complete: {} lines -> {} characters", lines.len(), result.len());
    debug!("üßπ Final VTT cleaning: {} -> {} characters", result.len(), final_result.len());
    debug!("üßπ Total reduction: {:.2}%", (final_result.len() as f64 / vtt.len() as f64) * 100.0);
    debug!("üßπ Final result preview: {}", &final_result[..std::cmp::min(100, final_result.len())]);
    
    trace!("üîç VTT cleaning final: original_length={}, processed_lines={}, kept_lines={}, final_length={}, reduction_percent={:.2}%", 
           vtt.len(), processed_lines, kept_lines, final_result.len(), 
           (final_result.len() as f64 / vtt.len() as f64) * 100.0);
    
    final_result
}

// Simple webpage fetcher with improved connectivity
// Downloads and cleans HTML content for a given URL using the shared HTTP client
async fn fetch_webpage_content(url: &str) -> Result<(String, String), Box<dyn std::error::Error + Send + Sync>> {
    let fetch_uuid = Uuid::new_v4();
    
    info!("üåê === WEBPAGE FETCHING STARTED ===");
    info!("üÜî Fetch UUID: {}", fetch_uuid);
    info!("üìç Target URL: {}", url);
    
    debug!("üîß === WEBPAGE FETCH INITIALIZATION ===");
    debug!("üîß URL length: {} characters", url.len());
    debug!("üîß Fetch UUID: {}", fetch_uuid);
    trace!("üîç Webpage fetch started: url_length={}, fetch_uuid={}", url.len(), fetch_uuid);
    
    debug!("üåê Starting webpage fetch for URL: {}", url);
    
    debug!("üîß === HTTP CLIENT SETUP ===");
    debug!("üîß Using shared HTTP client with optimized settings...");
    trace!("üîç HTTP client setup started: fetch_uuid={}", fetch_uuid);
    
    let client = get_http_client().await;
    
    debug!("‚úÖ Shared HTTP client obtained successfully");
    debug!("üîß Using optimized connection pooling and settings");
    trace!("üîç HTTP client obtained: fetch_uuid={}", fetch_uuid);
    
    debug!("üì° === HTTP REQUEST EXECUTION ===");
    debug!("üì° Sending HTTP request...");
    trace!("üîç HTTP request started: url={}, fetch_uuid={}", url, fetch_uuid);
    
    let response = client.get(url).send().await?;
    let status = response.status();
    
    debug!("üì° === HTTP RESPONSE RECEIVED ===");
    debug!("üì° HTTP Response Status: {}", status);
    debug!("üì° HTTP Response Status Code: {}", status.as_u16());
    debug!("üì° HTTP Response Success: {}", status.is_success());
    debug!("üì° HTTP Response Headers: {:?}", response.headers());
    trace!("üîç HTTP response received: status={}, status_code={}, success={}, fetch_uuid={}", 
           status, status.as_u16(), status.is_success(), fetch_uuid);
    
    if !response.status().is_success() {
        error!("‚ùå === HTTP ERROR RESPONSE ===");
        error!("‚ùå HTTP error: {}", status);
        debug!("üîç HTTP error details: status_code={}, status_text={}", status.as_u16(), status.as_str());
        trace!("üîç HTTP error: status={}, fetch_uuid={}", status, fetch_uuid);
        return Err(format!("HTTP error: {}", response.status()).into());
    }
    
    debug!("üìÑ === HTML CONTENT DOWNLOAD ===");
    debug!("üìÑ Downloading HTML content...");
    trace!("üîç HTML content download started: fetch_uuid={}", fetch_uuid);
    
    let html = response.text().await?;
    
    debug!("üìÑ === HTML CONTENT DOWNLOADED ===");
    debug!("üìÑ Downloaded HTML content: {} characters", html.len());
    debug!("üìÑ HTML content preview: {}", &html[..std::cmp::min(200, html.len())]);
    debug!("üìÑ HTML contains '<html': {}", html.contains("<html"));
    debug!("üìÑ HTML contains '<body': {}", html.contains("<body"));
    debug!("üìÑ HTML contains '<head': {}", html.contains("<head"));
    trace!("üîç HTML content downloaded: length={}, preview_length={}, fetch_uuid={}", 
           html.len(), std::cmp::min(200, html.len()), fetch_uuid);
    
    // Save HTML to temporary file for RAG processing
    debug!("üíæ === HTML FILE SAVING ===");
    debug!("üíæ Saving HTML content to temporary file...");
    trace!("üîç HTML file saving started: html_length={}, fetch_uuid={}", html.len(), fetch_uuid);
    
    let temp_dir = std::env::temp_dir();
    let file_name = format!("webpage_{}.html", fetch_uuid);
    let file_path = temp_dir.join(&file_name);
    
    debug!("üíæ Temporary file path: {:?}", file_path);
    debug!("üíæ File name: {}", file_name);
    trace!("üîç File path created: path={:?}, fetch_uuid={}", file_path, fetch_uuid);
    
    match fs::write(&file_path, &html) {
        Ok(_) => {
            debug!("‚úÖ HTML file saved successfully");
            debug!("üíæ File size: {} bytes", html.len());
            debug!("üíæ File path: {:?}", file_path);
            trace!("üîç HTML file saved: path={:?}, size={}, fetch_uuid={}", file_path, html.len(), fetch_uuid);
        },
        Err(e) => {
            error!("‚ùå === HTML FILE SAVE ERROR ===");
            error!("‚ùå Failed to save HTML file: {}", e);
            debug!("üîç File save error: path={:?}, error={}", file_path, e);
            trace!("üîç File save error: path={:?}, error_type={}, fetch_uuid={}", 
                   file_path, std::any::type_name_of_val(&e), fetch_uuid);
            return Err(format!("Failed to save HTML file: {}", e).into());
        }
    }
    
    // Basic HTML cleaning for immediate use
    debug!("üßπ === HTML CLEANING PHASE ===");
    debug!("üßπ Starting HTML content cleaning...");
    trace!("üîç HTML cleaning started: original_length={}, fetch_uuid={}", html.len(), fetch_uuid);
    
    let cleaned = clean_html(&html);
    
    debug!("‚úÖ === HTML CLEANING COMPLETED ===");
    debug!("‚úÖ HTML content cleaned: {} characters", cleaned.len());
    debug!("‚úÖ Cleaning ratio: {:.2}%", (cleaned.len() as f64 / html.len() as f64) * 100.0);
    debug!("‚úÖ Cleaned content preview: {}", &cleaned[..std::cmp::min(200, cleaned.len())]);
    trace!("üîç HTML cleaning completed: original_length={}, cleaned_length={}, reduction_percent={:.2}%, fetch_uuid={}", 
           html.len(), cleaned.len(), (cleaned.len() as f64 / html.len() as f64) * 100.0, fetch_uuid);
    
    info!("‚úÖ === WEBPAGE FETCHING COMPLETED ===");
    info!("‚úÖ Webpage content fetched, saved to file, and cleaned successfully");
    debug!("üìÑ Final content length: {} characters", cleaned.len());
    debug!("üíæ HTML file saved: {:?}", file_path);
    debug!("üìÑ Fetch UUID: {}", fetch_uuid);
    trace!("üîç Webpage fetch success: final_length={}, file_path={:?}, fetch_uuid={}", cleaned.len(), file_path, fetch_uuid);
    
    Ok((cleaned, file_path.to_string_lossy().to_string()))
}

// Simple HTML cleaner
// Removes script/style tags and all HTML tags, returns plain text
fn clean_html(html: &str) -> String {
    let clean_uuid = Uuid::new_v4();
    
    debug!("üßπ === HTML CLEANING STARTED ===");
    debug!("üÜî Clean UUID: {}", clean_uuid);
    debug!("üßπ Cleaning HTML content...");
    debug!("üßπ Original HTML length: {} characters", html.len());
    trace!("üîç HTML cleaning started: original_length={}, clean_uuid={}", html.len(), clean_uuid);
    
    // Remove script and style tags
    let mut result = html.to_string();
    let _original_result = result.clone();
    
    debug!("üßπ === SCRIPT TAG REMOVAL ===");
    debug!("üßπ Removing script tags...");
    let mut script_removals = 0;
    let mut script_removal_rounds = 0;
    
    // Remove script tags
    while let Some(start) = result.find("<script") {
        script_removal_rounds += 1;
        if let Some(end) = result[start..].find("</script>") {
            let script_content = &result[start..start + end + 9];
            script_removals += 1;
            debug!("üßπ Removed script tag {}: {} characters", script_removals, script_content.len());
            trace!("üîç Script removal: round={}, removal={}, script_length={}, clean_uuid={}", 
                   script_removal_rounds, script_removals, script_content.len(), clean_uuid);
            result.replace_range(start..start + end + 9, "");
        } else {
            debug!("üßπ Found incomplete script tag, stopping removal");
            trace!("üîç Incomplete script tag found: round={}, clean_uuid={}", script_removal_rounds, clean_uuid);
            break;
        }
    }
    
    debug!("‚úÖ Script tag removal completed: {} removals in {} rounds", script_removals, script_removal_rounds);
    
    debug!("üßπ === STYLE TAG REMOVAL ===");
    debug!("üßπ Removing style tags...");
    let mut style_removals = 0;
    let mut style_removal_rounds = 0;
    
    // Remove style tags
    while let Some(start) = result.find("<style") {
        style_removal_rounds += 1;
        if let Some(end) = result[start..].find("</style>") {
            let style_content = &result[start..start + end + 8];
            style_removals += 1;
            debug!("üßπ Removed style tag {}: {} characters", style_removals, style_content.len());
            trace!("üîç Style removal: round={}, removal={}, style_length={}, clean_uuid={}", 
                   style_removal_rounds, style_removals, style_content.len(), clean_uuid);
            result.replace_range(start..start + end + 8, "");
        } else {
            debug!("üßπ Found incomplete style tag, stopping removal");
            trace!("üîç Incomplete style tag found: round={}, clean_uuid={}", style_removal_rounds, clean_uuid);
            break;
        }
    }
    
    debug!("‚úÖ Style tag removal completed: {} removals in {} rounds", style_removals, style_removal_rounds);
    
    debug!("üßπ === HTML TAG REMOVAL ===");
    debug!("üßπ Removing all remaining HTML tags...");
    trace!("üîç HTML tag removal started: current_length={}, clean_uuid={}", result.len(), clean_uuid);
    
    // Remove all HTML tags
    let tag_regex = regex::Regex::new(r"<[^>]+>").unwrap();
    let cleaned = tag_regex.replace_all(&result, " ");
    
    debug!("‚úÖ HTML tag removal completed");
    debug!("üßπ Content after tag removal: {} characters", cleaned.len());
    debug!("üßπ Tag removal reduction: {:.2}%", (cleaned.len() as f64 / result.len() as f64) * 100.0);
    trace!("üîç HTML tag removal completed: before_length={}, after_length={}, reduction_percent={:.2}%, clean_uuid={}", 
           result.len(), cleaned.len(), (cleaned.len() as f64 / result.len() as f64) * 100.0, clean_uuid);
    
    debug!("üßπ === WHITESPACE CLEANUP ===");
    debug!("üßπ Cleaning whitespace...");
    let before_whitespace = cleaned.len();
    
    // Clean whitespace
    let final_result: String = cleaned
        .split_whitespace()
        .collect::<Vec<_>>()
        .join(" ")
        .chars()
        .take(15000)
        .collect();
    
    debug!("‚úÖ Whitespace cleanup completed");
    debug!("üßπ Content after whitespace cleanup: {} characters", final_result.len());
    debug!("üßπ Whitespace cleanup reduction: {:.2}%", (final_result.len() as f64 / before_whitespace as f64) * 100.0);
    debug!("üßπ Content truncated to 15000 characters: {}", final_result.len() >= 15000);
    trace!("üîç Whitespace cleanup: before_length={}, after_length={}, truncated={}, clean_uuid={}", 
           before_whitespace, final_result.len(), final_result.len() >= 15000, clean_uuid);
    
    debug!("üßπ === FINAL HTML CLEANING COMPLETED ===");
    debug!("üßπ HTML cleaning complete: {} -> {} characters", html.len(), final_result.len());
    debug!("üßπ Total reduction: {:.2}%", (final_result.len() as f64 / html.len() as f64) * 100.0);
    debug!("üßπ Final result preview: {}", &final_result[..std::cmp::min(100, final_result.len())]);
    debug!("üßπ Clean UUID: {}", clean_uuid);
    
    trace!("üîç HTML cleaning final: original_length={}, script_removals={}, style_removals={}, final_length={}, total_reduction_percent={:.2}%, clean_uuid={}", 
           html.len(), script_removals, style_removals, final_result.len(), 
           (final_result.len() as f64 / html.len() as f64) * 100.0, clean_uuid);
    
    final_result
}

// Stream summary using SSE (like lm command approach)
// Streams the AI's summary response, chunking and updating Discord messages as needed
async fn stream_summary(
    content: &str,
    url: &str,
    config: &LMConfig,
    selected_model: &str,
    msg: &mut Message,
    ctx: &Context,
    is_youtube: bool,
    file_path: Option<&str>,
) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    
    let stream_uuid = Uuid::new_v4();
    
    // Trace-level function entry
    trace!("[TRACE][SUM][stream_summary] === FUNCTION ENTRY ===");
    trace!("[TRACE][SUM][stream_summary] Function: stream_summary()");
    trace!("[TRACE][SUM][stream_summary] Stream UUID: {}", stream_uuid);
    trace!("[TRACE][SUM][stream_summary] Input content length: {} chars", content.len());
    trace!("[TRACE][SUM][stream_summary] URL: '{}'", url);
    trace!("[TRACE][SUM][stream_summary] Is YouTube: {}", is_youtube);
    trace!("[TRACE][SUM][stream_summary] File path: {:?}", file_path);
    trace!("[TRACE][SUM][stream_summary] Config base URL: {}", config.base_url);
    trace!("[TRACE][SUM][stream_summary] Selected model: {}", selected_model);
    trace!("[TRACE][SUM][stream_summary] Config temperature: {}", config.default_temperature);
    trace!("[TRACE][SUM][stream_summary] Config max tokens: {}", config.default_max_tokens);
    
    info!("ü§ñ === AI SUMMARIZATION STREAMING STARTED ===");
    info!("üÜî Stream UUID: {}", stream_uuid);
    info!("üåê URL: {}", url);
    info!("üì∫ Content type: {}", if is_youtube { "YouTube" } else { "Webpage" });
    info!("üìÑ Content length: {} characters", content.len());
    
    debug!("üîß === STREAM SUMMARY INITIALIZATION ===");
    debug!("üîß Stream UUID: {}", stream_uuid);
    debug!("üîß URL length: {} characters", url.len());
    debug!("üîß Content length: {} characters", content.len());
    debug!("üîß Is YouTube: {}", is_youtube);
    debug!("üîß File path: {:?}", file_path);
    debug!("üîß Model: {}", selected_model);
    debug!("üîß Base URL: {}", config.base_url);
    debug!("üîß Temperature: {}", config.default_temperature);
    debug!("üîß Max tokens: {}", config.default_max_tokens);
    trace!("üîç Stream summary started: content_length={}, url={}, is_youtube={}, model={}, stream_uuid={}", 
           content.len(), url, is_youtube, selected_model, stream_uuid);
    
    debug!("ü§ñ Preparing AI request...");
    trace!("üîç Stream summary started: content_length={}, url={}, is_youtube={}, model={}, stream_uuid={}", 
           content.len(), url, is_youtube, selected_model, stream_uuid);    
    
    // Load appropriate system prompt from files
    debug!("üìÑ === SYSTEM PROMPT LOADING ===");
    debug!("üìÑ Loading system prompt for content type: {}", if is_youtube { "YouTube" } else { "Webpage" });
    trace!("üîç Loading system prompt: is_youtube={}, stream_uuid={}", is_youtube, stream_uuid);
    
    let system_prompt = if is_youtube {
        debug!("üì∫ Loading YouTube summarization prompt...");
        match load_youtube_summarization_prompt().await {
            Ok(prompt) => {
                debug!("‚úÖ YouTube summarization prompt loaded: {} characters", prompt.len());
                trace!("üîç YouTube prompt loaded: length={}, stream_uuid={}", prompt.len(), stream_uuid);
                prompt
            },
            Err(e) => {
                error!("‚ùå Failed to load YouTube summarization prompt: {}", e);
                debug!("üîç YouTube prompt error: {:?}", e);
                trace!("üîç YouTube prompt error: error_type={}, stream_uuid={}", 
                       std::any::type_name_of_val(&e), stream_uuid);
                return Err(e);
            }
        }
    } else {
        debug!("üìÑ Loading general summarization prompt...");
        match load_summarization_prompt().await {
            Ok(prompt) => {
                debug!("‚úÖ General summarization prompt loaded: {} characters", prompt.len());
                trace!("üîç General prompt loaded: length={}, stream_uuid={}", prompt.len(), stream_uuid);
                prompt
            },
            Err(e) => {
                error!("‚ùå Failed to load general summarization prompt: {}", e);
                debug!("üîç General prompt error: {:?}", e);
                trace!("üîç General prompt error: error_type={}, stream_uuid={}", 
                       std::any::type_name_of_val(&e), stream_uuid);
                return Err(e);
            }
        }
    };
    
    debug!("üìÑ System prompt loaded successfully: {} characters", system_prompt.len());
    debug!("üìÑ System prompt preview: {}", &system_prompt[..std::cmp::min(200, system_prompt.len())]);
    trace!("üîç System prompt loaded: length={}, stream_uuid={}", system_prompt.len(), stream_uuid);
    
    // FIXED: Properly handle content processing for YouTube vs webpage
    trace!("[TRACE][SUM][stream_summary] === CONTENT PROCESSING ENTRY ===");
    trace!("[TRACE][SUM][stream_summary] File path provided: {}", file_path.is_some());
    trace!("[TRACE][SUM][stream_summary] Content type: {}", if is_youtube { "YouTube" } else { "Webpage" });
    trace!("[TRACE][SUM][stream_summary] Input content length: {} chars", content.len());
    
    debug!("üîß === CONTENT PROCESSING ===");
    debug!("üîß Processing content for AI request...");
    
    let (user_prompt, content_to_process) = if file_path.is_some() {
        // Use RAG document processing with the file (YouTube subtitle or HTML)
        let file_path = file_path.unwrap();
        debug!("üîß === RAG PROCESSING TRIGGERED ===");
        debug!("üîß File path provided: {}", file_path);
        debug!("üîß File exists: {}", std::path::Path::new(file_path).exists());
        debug!("üîß Is YouTube: {}", is_youtube);
        debug!("üìÅ === RAG DOCUMENT PROCESSING ===");
        debug!("üìÅ Using RAG document processing for file: {}", file_path);
        debug!("üìÅ Content type: {}", if is_youtube { "YouTube subtitle" } else { "HTML webpage" });
        trace!("üîç RAG document processing: file_path={}, content_type={}, stream_uuid={}", 
               file_path, if is_youtube { "youtube" } else { "webpage" }, stream_uuid);
        
        // Enhanced logging for RAG processing
        log::info!("üìÅ === RAG PROCESSING STARTED ===");
        log::info!("üìÅ File path: {}", file_path);
        log::info!("üìÅ Content type: {}", if is_youtube { "YouTube subtitle" } else { "HTML webpage" });
        log::info!("üìÅ File exists: {}", std::path::Path::new(file_path).exists());
        log::info!("üìÅ Stream UUID: {}", stream_uuid);
        
        // Read the file content
        debug!("üìñ === FILE READING FOR RAG ===");
        debug!("üìñ Reading file for RAG processing: {}", file_path);
        let file_content = match fs::read_to_string(file_path) {
            Ok(content) => {
                debug!("‚úÖ File read successfully: {} characters", content.len());
                debug!("üìñ File content preview: {}", &content[..std::cmp::min(200, content.len())]);
                trace!("üîç File read success: path={}, length={}, stream_uuid={}", file_path, content.len(), stream_uuid);
                
                // Enhanced logging for file reading success
                log::info!("üìñ === FILE READ SUCCESS ===");
                log::info!("üìñ File path: {}", file_path);
                log::info!("üìñ File size: {} characters", content.len());
                log::info!("üìñ File size in bytes: {} bytes", content.as_bytes().len());
                log::info!("üìñ Content preview: {}", &content[..std::cmp::min(500, content.len())]);
                log::info!("üìñ Content type indicators:");
                log::info!("üìñ   - Contains HTML tags: {}", content.contains("<html"));
                log::info!("üìñ   - Contains VTT timestamps: {}", content.contains("-->"));
                log::info!("üìñ   - Contains script tags: {}", content.contains("<script"));
                log::info!("üìñ   - Contains style tags: {}", content.contains("<style"));
                
                content
            },
            Err(e) => {
                error!("‚ùå === FILE READ ERROR ===");
                error!("‚ùå Failed to read file: {}", e);
                debug!("üîç File read error: path={}, error={}", file_path, e);
                debug!("üîç File read error type: {:?}", std::any::type_name_of_val(&e));
                trace!("üîç File read error: path={}, error_type={}, stream_uuid={}", 
                       file_path, std::any::type_name_of_val(&e), stream_uuid);
                return Err(format!("Failed to read file: {}", e).into());
            }
        };
        
        // Clean the content based on file type
        let cleaned_content = if is_youtube {
            debug!("üßπ === VTT CLEANING FOR RAG ===");
            debug!("üßπ Cleaning VTT content for RAG processing...");
            trace!("üîç VTT cleaning for RAG: original_length={}, stream_uuid={}", file_content.len(), stream_uuid);
            
            let cleaned = clean_vtt_content(&file_content);
            
            debug!("‚úÖ VTT content cleaned for RAG: {} characters", cleaned.len());
            debug!("üßπ Content preview: {}", &cleaned[..std::cmp::min(200, cleaned.len())]);
            debug!("üßπ Cleaning ratio: {:.2}%", (cleaned.len() as f64 / file_content.len() as f64) * 100.0);
            trace!("üîç VTT cleaning for RAG completed: original_length={}, cleaned_length={}, stream_uuid={}", 
                   file_content.len(), cleaned.len(), stream_uuid);
            
            // Enhanced logging for VTT cleaning
            log::info!("üßπ === VTT CLEANING COMPLETED ===");
            log::info!("üßπ Original size: {} characters", file_content.len());
            log::info!("üßπ Cleaned size: {} characters", cleaned.len());
            log::info!("üßπ Reduction: {:.2}%", (cleaned.len() as f64 / file_content.len() as f64) * 100.0);
            log::info!("üßπ Cleaned preview: {}", &cleaned[..std::cmp::min(400, cleaned.len())]);
            
            cleaned
        } else {
            debug!("üßπ === HTML CLEANING FOR RAG ===");
            debug!("üßπ Cleaning HTML content for RAG processing...");
            trace!("üîç HTML cleaning for RAG: original_length={}, stream_uuid={}", file_content.len(), stream_uuid);
            
            let cleaned = clean_html(&file_content);
            
            debug!("‚úÖ HTML content cleaned for RAG: {} characters", cleaned.len());
            debug!("üßπ Content preview: {}", &cleaned[..std::cmp::min(200, cleaned.len())]);
            debug!("üßπ Cleaning ratio: {:.2}%", (cleaned.len() as f64 / file_content.len() as f64) * 100.0);
            trace!("üîç HTML cleaning for RAG completed: original_length={}, cleaned_length={}, stream_uuid={}", 
                   file_content.len(), cleaned.len(), stream_uuid);
            
            // Enhanced logging for HTML cleaning
            log::info!("üßπ === HTML CLEANING COMPLETED ===");
            log::info!("üßπ Original size: {} characters", file_content.len());
            log::info!("üßπ Cleaned size: {} characters", cleaned.len());
            log::info!("üßπ Reduction: {:.2}%", (cleaned.len() as f64 / file_content.len() as f64) * 100.0);
            log::info!("üßπ Cleaned preview: {}", &cleaned[..std::cmp::min(400, cleaned.len())]);
            log::info!("üßπ Word count: {} words", cleaned.split_whitespace().count());
            
            cleaned
        };
        
        // Verify we have actual content
        if cleaned_content.trim().is_empty() {
            error!("‚ùå === EMPTY CLEANED CONTENT ERROR ===");
            error!("‚ùå Cleaned content is empty");
            debug!("üîç Cleaned content empty: original_length={}, cleaned_length={}", file_content.len(), cleaned_content.len());
            trace!("üîç Cleaned content empty: stream_uuid={}", stream_uuid);
            return Err("File contains no readable content after cleaning".into());
        }
        
        let prompt = format!(
            "Please analyze and summarize this {} from {}:\n\n{}",
            if is_youtube { "YouTube video subtitle file" } else { "webpage HTML content" },
            url, cleaned_content
        );
        
        debug!("üìù === USER PROMPT CREATION FOR RAG ===");
        debug!("üìù Created user prompt with {} content: {} characters", 
               if is_youtube { "subtitle" } else { "HTML" }, prompt.len());
        debug!("üìù Prompt preview: {}", &prompt[..std::cmp::min(300, prompt.len())]);
        trace!("üîç User prompt created: prompt_length={}, cleaned_content_length={}, content_type={}, stream_uuid={}", 
               prompt.len(), cleaned_content.len(), if is_youtube { "youtube" } else { "webpage" }, stream_uuid);
        
        (prompt, cleaned_content)
    } else {
        // For webpages or fallback, use the original content
        debug!("üîß === DIRECT PROCESSING TRIGGERED ===");
        debug!("üîß No file path provided, using direct content processing");
        debug!("üîß Content length: {} characters", content.len());
        debug!("üîß Is YouTube: {}", is_youtube);
        debug!("üìÑ === WEBPAGE CONTENT PROCESSING ===");
        debug!("üìÑ Using webpage content processing");
        trace!("üîç Webpage content processing: content_length={}, stream_uuid={}", content.len(), stream_uuid);
        
        // For YouTube videos, don't truncate content since RAG will handle chunking
        // For webpages, apply reasonable limits to prevent context overflow
        let max_content_length = if is_youtube { 
            usize::MAX // No limit for YouTube videos - RAG will handle chunking
        } else { 
            20000 // Limit for webpages
        };
        
        debug!("üìè === CONTENT LENGTH CHECK ===");
        debug!("üìè Checking content length: {} characters", content.len());
        debug!("üìè Max content length: {} characters", max_content_length);
        debug!("üìè Content type: {}", if is_youtube { "YouTube" } else { "Webpage" });
        debug!("üìè Needs truncation: {}", content.len() > max_content_length);
        trace!("üîç Content length check: content_length={}, max_length={}, content_type={}, stream_uuid={}", 
               content.len(), max_content_length, if is_youtube { "youtube" } else { "webpage" }, stream_uuid);
        
        let truncated_content = if content.len() > max_content_length {
            let truncated = format!("{} [Content truncated due to length]", &content[0..max_content_length]);
            debug!("üìè Content truncated: {} -> {} characters", content.len(), truncated.len());
            debug!("üìè Truncation reduction: {:.2}%", (truncated.len() as f64 / content.len() as f64) * 100.0);
            trace!("üîç Content truncated: original_length={}, truncated_length={}, stream_uuid={}", 
                   content.len(), truncated.len(), stream_uuid);
            truncated
        } else {
            debug!("üìè Content length is within limits, no truncation needed");
            trace!("üîç Content within limits: length={}, stream_uuid={}", content.len(), stream_uuid);
            content.to_string()
        };
        
        let prompt = format!(
            "Please summarize this {} from {}:\n\n{}",
            if is_youtube { "YouTube video transcript" } else { "webpage content" },
            url,
            truncated_content
        );
        
        debug!("üìù === USER PROMPT CREATION FOR WEBPAGE ===");
        debug!("üìù Created user prompt with webpage content: {} characters", prompt.len());
        debug!("üìù Prompt preview: {}", &prompt[..std::cmp::min(300, prompt.len())]);
        trace!("üîç User prompt created: prompt_length={}, truncated_content_length={}, stream_uuid={}", 
               prompt.len(), truncated_content.len(), stream_uuid);
        
        (prompt, truncated_content)
    };
    
    debug!("üìù === PROMPT SUMMARY ===");
    debug!("üìù System prompt length: {} characters", system_prompt.len());
    debug!("üìù User prompt length: {} characters", user_prompt.len());
    debug!("üìù Content to process length: {} characters", content_to_process.len());
    debug!("üìù Total prompt length: {} characters", system_prompt.len() + user_prompt.len());
    trace!("üîç Prompt details: system_length={}, user_length={}, content_length={}, url_length={}, stream_uuid={}", 
           system_prompt.len(), user_prompt.len(), content_to_process.len(), url.len(), stream_uuid);
    
    // Use model context limit of 32,000 tokens, with safety margin for prompts
    // Assuming ~4 characters per token, use ~24,000 characters per chunk to leave room for prompts
    let chunk_size = if is_youtube { 24000 } else { 16000 }; // Optimized for 32K context limit
    let mut chunk_summaries = Vec::new();
    let request_payload;
    
            debug!("üìÑ === CHUNKING DECISION ===");
        debug!("üìÑ Content length: {} characters", content_to_process.len());
        debug!("üìÑ Chunk size: {} characters (optimized for 32K context)", chunk_size);
        debug!("üìÑ Model context limit: 32,000 tokens");
        debug!("üìÑ Max tokens per response: {}", config.default_max_tokens);
        debug!("üìÑ Needs chunking: {}", content_to_process.len() > chunk_size);
    trace!("üîç Chunking decision: content_length={}, chunk_size={}, needs_chunking={}, stream_uuid={}", 
           content_to_process.len(), chunk_size, content_to_process.len() > chunk_size, stream_uuid);
    
    if content_to_process.len() > chunk_size {
        info!("üìÑ === RAG SUMMARIZATION STARTED ===");
        info!("üìÑ Content too long ({} chars), using map-reduce RAG summarization", content_to_process.len());
        debug!("üìÑ Starting RAG summarization with chunking...");
        trace!("üîç RAG summarization started: content_length={}, chunk_size={}, stream_uuid={}", 
               content_to_process.len(), chunk_size, stream_uuid);
        
        // Enhanced logging for chunking process
        log::info!("üìÑ === CHUNKING PROCESS STARTED ===");
        log::info!("üìÑ Content length: {} characters", content_to_process.len());
        log::info!("üìÑ Chunk size: {} characters", chunk_size);
        log::info!("üìÑ Estimated chunks: {}", (content_to_process.len() + chunk_size - 1) / chunk_size);
        log::info!("üìÑ Content type: {}", if is_youtube { "YouTube" } else { "Webpage" });
        log::info!("üìÑ Processing method: Map-reduce RAG summarization");
        
        // FIXED: Proper character-based chunking of the actual content
        debug!("üìÑ === CONTENT CHUNKING ===");
        debug!("üìÑ Splitting content into chunks using character-based splitting...");
        
        // Use character-based splitting to avoid breaking UTF-8 characters
        let mut chunks = Vec::new();
        let mut current_chunk = String::new();
        let words: Vec<&str> = content_to_process.split_whitespace().collect();
        
        // Safety check for extremely long content
        if words.len() > 100000 {
            warn!("‚ö†Ô∏è === EXTREMELY LONG CONTENT WARNING ===");
            warn!("‚ö†Ô∏è Content has {} words, this may cause performance issues", words.len());
        }
        
        for word in words {
            // Check if a single word is too long (might be corrupted data)
            if word.len() > chunk_size / 2 {
                warn!("‚ö†Ô∏è Skipping extremely long word: {} characters", word.len());
                continue;
            }
            
            if current_chunk.len() + word.len() + 1 > chunk_size && !current_chunk.is_empty() {
                chunks.push(current_chunk.trim().to_string());
                current_chunk = String::new();
            }
            
            if !current_chunk.is_empty() {
                current_chunk.push(' ');
            }
            current_chunk.push_str(word);
        }
        
        // Add the last chunk if it's not empty
        if !current_chunk.is_empty() {
            chunks.push(current_chunk.trim().to_string());
        }
        
        // Safety check for too many chunks
        if chunks.len() > 50 {
            warn!("‚ö†Ô∏è === TOO MANY CHUNKS WARNING ===");
            warn!("‚ö†Ô∏è Content split into {} chunks, this may take a very long time", chunks.len());
        }
        
        debug!("üìÑ Split content into {} chunks", chunks.len());
        debug!("üìÑ Chunk sizes: {:?}", chunks.iter().map(|c| c.len()).collect::<Vec<_>>());
        trace!("üîç Content chunked: total_chunks={}, stream_uuid={}", chunks.len(), stream_uuid);
        
        // For very long videos, implement hierarchical summarization
        let is_very_long_video = is_youtube && chunks.len() > 10;
        if is_very_long_video {
            info!("üìÑ === VERY LONG VIDEO DETECTED ===");
            info!("üìÑ Video has {} chunks, using hierarchical summarization", chunks.len());
            debug!("üìÑ Implementing hierarchical summarization for very long video");
        }
        
        for (i, chunk) in chunks.iter().enumerate() {
            info!("ü§ñ === CHUNK {} PROCESSING ===", i+1);
            info!("ü§ñ Summarizing chunk {} of {} ({} chars)", i+1, chunks.len(), chunk.len());
            debug!("ü§ñ Chunk {} preview: {}", i+1, &chunk[..std::cmp::min(100, chunk.len())]);
            trace!("üîç Chunk {} processing: chunk_length={}, stream_uuid={}", i+1, chunk.len(), stream_uuid);
            
            // FIXED: Create a more specific prompt for each chunk with actual content
            let chunk_prompt = format!(
                "Create a detailed summary of this content chunk from {}. Focus on key points, topics, and important information:\n\n{}",
                if is_youtube { "a YouTube video" } else { "a webpage" },
                chunk
            );
            
            debug!("üìù === CHUNK PROMPT CREATION ===");
            debug!("üìù Created chunk prompt: {} characters", chunk_prompt.len());
            debug!("üìù Chunk prompt preview: {}", &chunk_prompt[..std::cmp::min(200, chunk_prompt.len())]);
            trace!("üîç Chunk prompt created: chunk={}, prompt_length={}, stream_uuid={}", 
                   i+1, chunk_prompt.len(), stream_uuid);
            
            let chunk_messages = vec![
                ChatMessage { 
                    role: "system".to_string(), 
                    content: "You are an expert content summarizer. Create comprehensive summaries that capture all important details, key points, and main topics from the provided content. Aim for summaries that are informative and detailed while remaining concise.".to_string() 
                },
                ChatMessage { 
                    role: "user".to_string(), 
                    content: chunk_prompt.clone() 
                }
            ];
            
            debug!("ü§ñ === CHUNK LLM REQUEST ===");
            debug!("ü§ñ Sending chunk {} to LLM with {} characters", i+1, chunk.len());
            debug!("ü§ñ Using selected model: {}", selected_model);
            debug!("ü§ñ Max tokens: 2000 (optimized for 32K context)");
            trace!("üîç Chunk {} LLM request: chunk_length={}, prompt_length={}, model={}, stream_uuid={}", 
                   i+1, chunk.len(), chunk_prompt.len(), selected_model, stream_uuid);
            
            // Use selected model with retry logic for chunk summaries
            let chunk_summary = match chat_completion(chunk_messages, selected_model, config, Some(2000)).await {
                Ok(summary) => {
                    debug!("‚úÖ Chunk {} summary received: {} characters", i+1, summary.len());
                    debug!("üìù Chunk {} summary preview: {}", i+1, &summary[..std::cmp::min(200, summary.len())]);
                    trace!("üîç Chunk {} summary completed: summary_length={}, stream_uuid={}", 
                           i+1, summary.len(), stream_uuid);
                    summary
                },
                Err(e) => {
                    error!("‚ùå === CHUNK {} LLM ERROR ===", i+1);
                    error!("‚ùå Failed to get summary for chunk {}: {}", i+1, e);
                    debug!("üîç Chunk {} LLM error: {:?}", i+1, e);
                    debug!("üîç Chunk {} LLM error type: {:?}", i+1, std::any::type_name_of_val(&e));
                    trace!("üîç Chunk {} LLM error: error_type={}, stream_uuid={}", 
                           i+1, std::any::type_name_of_val(&e), stream_uuid);
                    
                    // Enhanced error handling with user-friendly messages
                    let error_msg = format!("{}", e);
                    if error_msg.contains("Connection refused") || error_msg.contains("Connection Error") {
                        return Err(format!(
                            "‚ùå **LM Studio Connection Lost During Chunk Processing**\n\n\
                            Failed to process chunk {} of {}\n\n\
                            **Solutions:**\n\
                            ‚Ä¢ **Check LM Studio**: Ensure LM Studio is still running\n\
                            ‚Ä¢ **Model Loaded**: Verify model `{}` is still loaded\n\
                            ‚Ä¢ **Server Status**: Check if LM Studio server is still active\n\
                            ‚Ä¢ **Try Shorter Content**: Consider using shorter videos/webpages\n\n\
                            **Progress:** Successfully processed {} of {} chunks before failure",
                            i+1, chunks.len(), selected_model, i, chunks.len()
                        ).into());
                    } else if error_msg.contains("Windows Permission Error") || error_msg.contains("os error 10013") {
                        return Err(format!(
                            "‚ùå **Windows Permission Error During Processing**\n\n\
                            Network access was denied while processing chunk {} of {}\n\n\
                            **Quick Fix:**\n\
                            ‚Ä¢ **Restart as Administrator**: Close the bot and run as administrator\n\n\
                            **Progress:** Successfully processed {} of {} chunks before failure",
                            i+1, chunks.len(), i, chunks.len()
                        ).into());
                    } else {
                        return Err(format!(
                            "‚ùå **Chunk Processing Error**\n\n\
                            Failed to process chunk {} of {}\n\n\
                            **Error:** {}\n\n\
                            **Progress:** Successfully processed {} of {} chunks\n\
                            **Suggestion:** Try using shorter content or a different model",
                            i+1, chunks.len(), e, i, chunks.len()
                        ).into());
                    }
                }
            };
            
            // Check if the model returned a fallback message
            debug!("üîç === CHUNK SUMMARY VALIDATION ===");
            debug!("üîç Checking chunk {} summary for fallback messages...", i+1);
            debug!("üîç Contains 'Search functionality is not available': {}", chunk_summary.contains("Search functionality is not available"));
            debug!("üîç Contains 'fallback': {}", chunk_summary.contains("fallback"));
            
            if chunk_summary.contains("Search functionality is not available") || chunk_summary.contains("fallback") {
                warn!("‚ö†Ô∏è === CHUNK {} FALLBACK DETECTED ===", i+1);
                warn!("‚ö†Ô∏è Model {} appears to be a search model, not suitable for summarization", selected_model);
                debug!("üîç Chunk {} returned search model response: {}", i+1, chunk_summary);
                debug!("üîç Using direct content approach for this chunk");
                // Use a more direct approach for this chunk
                let direct_summary = format!("Content chunk {}: {}", i+1, chunk);
                                chunk_summaries.push(direct_summary.clone());
                trace!("üîç Chunk {} fallback used: direct_summary_length={}, stream_uuid={}",        
                       i+1, direct_summary.len(), stream_uuid);
            } else {
                                chunk_summaries.push(chunk_summary.clone());
                trace!("üîç Chunk {} summary added: summary_length={}, stream_uuid={}",
                       i+1, chunk_summary.len(), stream_uuid);
            }
        }
        
        // FIXED: Combine chunk summaries for final prompt with better structure
        debug!("üìù === CHUNK SUMMARIES COMBINATION ===");
        debug!("üìù Combining {} chunk summaries...", chunk_summaries.len());
        
        let combined = if is_very_long_video {
            // For very long videos, implement hierarchical summarization
            info!("üìÑ === HIERARCHICAL SUMMARIZATION FOR VERY LONG VIDEO ===");
            info!("üìÑ Processing {} chunk summaries hierarchically", chunk_summaries.len());
            
            // Group chunk summaries into sections (every 5 chunks)
            let mut section_summaries = Vec::new();
            let section_size = 5;
            
            for (section_idx, section_chunks) in chunk_summaries.chunks(section_size).enumerate() {
                info!("üìÑ === SECTION {} SUMMARIZATION ===", section_idx + 1);
                info!("üìÑ Summarizing section {} with {} chunks", section_idx + 1, section_chunks.len());
                
                let section_combined = section_chunks.join("\n\n---\n\n");
                debug!("üìù Section {} combined: {} characters", section_idx + 1, section_combined.len());
                
                let section_prompt = format!(
                    "Create a comprehensive summary of this section from a YouTube video. Focus on the main topics, key points, and important information:\n\n{}",
                    section_combined
                );
                
                let section_messages = vec![
                    ChatMessage { 
                        role: "system".to_string(), 
                        content: "You are an expert content summarizer. Create comprehensive section summaries that capture the main topics and key information.".to_string() 
                    },
                    ChatMessage { 
                        role: "user".to_string(), 
                        content: section_prompt 
                    }
                ];
                
                let section_summary = match chat_completion(section_messages, selected_model, config, Some(1500)).await {
                    Ok(summary) => {
                        info!("‚úÖ Section {} summary completed: {} characters", section_idx + 1, summary.len());
                        summary
                    },
                    Err(e) => {
                        error!("‚ùå Failed to summarize section {}: {}", section_idx + 1, e);
                        
                        // Enhanced error handling for section processing
                        let error_msg = format!("{}", e);
                        if error_msg.contains("Connection refused") || error_msg.contains("Connection Error") {
                            return Err(format!(
                                "‚ùå **LM Studio Connection Lost During Section Processing**\n\n\
                                Failed to process section {} during hierarchical summarization\n\n\
                                **Solutions:**\n\
                                ‚Ä¢ **Check LM Studio**: Ensure LM Studio is still running\n\
                                ‚Ä¢ **Model Status**: Verify model `{}` is still loaded\n\
                                ‚Ä¢ **Try Again**: Restart the command\n\n\
                                **Progress:** Individual chunks completed, failed during section consolidation",
                                section_idx + 1, selected_model
                            ).into());
                        } else {
                            return Err(format!(
                                "‚ùå **Section Processing Error**\n\n\
                                Failed during hierarchical summarization of section {}\n\n\
                                **Error:** {}\n\n\
                                **Note:** Individual chunks were processed successfully",
                                section_idx + 1, e
                            ).into());
                        }
                    }
                };
                
                section_summaries.push(format!("Section {}: {}", section_idx + 1, section_summary));
            }
            
            // Combine section summaries for final summary
            let final_sections = section_summaries.join("\n\n---\n\n");
            debug!("üìù Final sections combined: {} characters", final_sections.len());
            final_sections
        } else {
            // For normal videos, use direct combination
            let combined = chunk_summaries.join("\n\n---\n\n");
            debug!("üìù Combined chunk summaries: {} characters", combined.len());
            debug!("üìù Combined summaries preview: {}", &combined[..std::cmp::min(300, combined.len())]);
            trace!("üîç Chunk summaries combined: combined_length={}, chunk_count={}, stream_uuid={}", 
                   combined.len(), chunk_summaries.len(), stream_uuid);
            combined
        };
        
        // Limit final prompt size to prevent context overflow
        let max_final_prompt_size = 80000; // ~20K tokens
        let final_content = if combined.len() > max_final_prompt_size {
            warn!("‚ö†Ô∏è === FINAL PROMPT TOO LARGE ===");
            warn!("‚ö†Ô∏è Combined content too large ({} chars), truncating to {} chars", combined.len(), max_final_prompt_size);
            format!("{} [Content truncated due to size - showing first {} characters]", 
                    &combined[0..max_final_prompt_size], max_final_prompt_size)
        } else {
            combined
        };
        
        let final_user_prompt = format!(
            "Create a comprehensive, well-structured summary of this {} from {}. Use the following detailed chunk summaries to build a complete overview that covers all major topics, key points, and important information:\n\n{}\n\nPlease organize the summary with clear sections and highlight the most important takeaways.",
            if is_youtube { "YouTube video" } else { "webpage" },
            url, final_content
        );
        
        debug!("üìù === FINAL RAG PROMPT CREATION ===");
        debug!("üìù Created final RAG prompt: {} characters", final_user_prompt.len());
        debug!("üìù Final prompt preview: {}", &final_user_prompt[..std::cmp::min(300, final_user_prompt.len())]);
        trace!("üîç Final RAG prompt created: final_prompt_length={}, stream_uuid={}", final_user_prompt.len(), stream_uuid);
        
        let final_messages = vec![
            ChatMessage { role: "system".to_string(), content: system_prompt.clone() },
            ChatMessage { role: "user".to_string(), content: final_user_prompt.clone() }
        ];
        
        debug!("üìù Final RAG prompt created: {} characters", final_user_prompt.len());
        debug!("üìù Final message count: {}", final_messages.len());
        trace!("üîç Final RAG prompt created: final_prompt_length={}, message_count={}, stream_uuid={}", 
               final_user_prompt.len(), final_messages.len(), stream_uuid);
        
        request_payload = serde_json::json!(
            {
                "model": selected_model,
                "messages": final_messages,
                "temperature": config.default_temperature,
                "max_tokens": config.default_max_tokens,
                "stream": true
            }
        );
    } else {
        info!("üìÑ === DIRECT SUMMARIZATION ===");
        info!("üìÑ Content length ({}) is within limits, using direct summarization", content_to_process.len());
        debug!("üìÑ Using direct summarization approach for {}", if is_youtube { "YouTube" } else { "webpage" });
        trace!("üîç Direct summarization: content_length={}, chunk_size={}, content_type={}, stream_uuid={}", 
               content_to_process.len(), chunk_size, if is_youtube { "youtube" } else { "webpage" }, stream_uuid);
        
        let messages = vec![
            ChatMessage {
                role: "system".to_string(),
                content: system_prompt,
            },
            ChatMessage {
                role: "user".to_string(),
                content: user_prompt,
            },
        ];
        
        debug!("üìù === DIRECT PROMPT CREATION ===");
        debug!("üìù Created direct summarization messages for {}: {} messages", if is_youtube { "YouTube" } else { "webpage" }, messages.len());
        debug!("üìù System message length: {} characters", messages[0].content.len());
        debug!("üìù User message length: {} characters", messages[1].content.len());
        debug!("üìù Total message length: {} characters", messages[0].content.len() + messages[1].content.len());
        trace!("üîç Direct summarization: message_count={}, system_length={}, user_length={}, content_type={}, stream_uuid={}", 
               messages.len(), messages[0].content.len(), messages[1].content.len(), if is_youtube { "youtube" } else { "webpage" }, stream_uuid);
        
        request_payload = serde_json::json!(
            {
                "model": selected_model,
                "messages": messages,
                "temperature": config.default_temperature,
                "max_tokens": config.default_max_tokens,
                "stream": true
            }
        );
    }
    
    // Use shared HTTP client with optimal settings
    debug!("üîß === HTTP CLIENT SETUP ===");
    debug!("üîß Using shared HTTP client for streaming request...");
    trace!("üîç HTTP client setup started: stream_uuid={}", stream_uuid);
    
    // Calculate appropriate timeout for content complexity
    let timeout_seconds = 300; // 5 minutes for all content types
    
    let client = get_http_client().await;
    
    debug!("‚úÖ Shared HTTP client obtained successfully");
    debug!("üîß Using optimized timeout: {} seconds", timeout_seconds);
    debug!("üîß Content length: {} characters", content_to_process.len());
    debug!("üîß Video type: {}", if is_youtube { "YouTube" } else { "Webpage" });
    debug!("üîß Connection pooling and SSL bypass enabled");
    trace!("üîç HTTP client obtained: timeout={}s, content_length={}, stream_uuid={}", timeout_seconds, content_to_process.len(), stream_uuid);
    
    // Send streaming request
    let api_url = format!("{}/v1/chat/completions", config.base_url);
    let payload_size = serde_json::to_string(&request_payload).unwrap_or_default().len();
    
    debug!("üöÄ === STREAMING REQUEST PREPARATION ===");
    debug!("üöÄ API URL: {}", api_url);
    debug!("üöÄ Payload size: {} bytes", payload_size);
    debug!("üöÄ Model: {}", selected_model);
    debug!("üöÄ Temperature: {}", config.default_temperature);
    debug!("üöÄ Max tokens: {}", config.default_max_tokens);
    debug!("üöÄ Streaming: true");
    trace!("üîç API request preparation: url={}, payload_size={}, stream_uuid={}", api_url, payload_size, stream_uuid);
    
    debug!("üöÄ Sending streaming request to LLM...");
    trace!("üîç Streaming request started: stream_uuid={}", stream_uuid);
    
    let mut response = match client
        .post(&api_url)
        .json(&request_payload)
        .timeout(Duration::from_secs(timeout_seconds))
        .send()
        .await {
            Ok(resp) => resp,
            Err(e) => {
                error!("‚ùå === HTTP REQUEST ERROR ===");
                error!("‚ùå Failed to send HTTP request: {}", e);
                
                let error_msg = format!("{}", e);
                let error_message = if e.is_timeout() {
                    format!("‚è∞ **Request Timeout**\n\nThe request to LM Studio timed out after {} seconds.\n\n**Solutions:**\n‚Ä¢ **Reduce Content**: Try a shorter video/webpage\n‚Ä¢ **Check LM Studio**: Ensure LM Studio is running and responsive\n‚Ä¢ **Model Performance**: Consider using a faster model\n‚Ä¢ **System Resources**: Check if your system has enough RAM/CPU\n\n**Current Setup:**\n‚Ä¢ LM Studio URL: `{}`\n‚Ä¢ Model: `{}`\n‚Ä¢ Content Length: {} characters\n\n*Source: <{}>*", 
                            timeout_seconds, config.base_url, selected_model, content_to_process.len(), url)
                } else if e.is_connect() {
                                          format!("üö´ **Connection Error**\n\nCannot connect to LM Studio at `{}`.\n\n**Solutions:**\n‚Ä¢ **Start LM Studio**: Ensure LM Studio is running\n‚Ä¢ **Load Model**: Load model `{}` in LM Studio\n‚Ä¢ **Enable Server**: Click 'Start Server' in LM Studio\n‚Ä¢ **Check Configuration**: Verify URL in lmapiconf.txt\n‚Ä¢ **Firewall**: Check Windows Defender/firewall settings\n‚Ä¢ **Try localhost**: Use `http://127.0.0.1:1234` instead of localhost\n\n*Source: <{}>*", 
                            config.base_url, selected_model, url)
                } else if error_msg.contains("os error 10013") {
                    format!("üö´ **Windows Permission Error (10013)**\n\nNetwork access denied by Windows.\n\n**Quick Fixes:**\n‚Ä¢ **Run as Administrator**: Right-click and 'Run as administrator'\n‚Ä¢ **Windows Firewall**: Add firewall exception for this program\n‚Ä¢ **Use IP Address**: Try `http://127.0.0.1:1234` in lmapiconf.txt\n\n**Current URL:** `{}`\n*Source: <{}>*", 
                            config.base_url, url)
                } else {
                    format!("üîó **Network Error**\n\nUnexpected network error occurred.\n\n**Error:** {}\n\n**Solutions:**\n‚Ä¢ **Check Connection**: Verify your internet connection\n‚Ä¢ **Restart LM Studio**: Try restarting LM Studio\n‚Ä¢ **Check Configuration**: Verify settings in lmapiconf.txt\n‚Ä¢ **Try Again**: Network issues are often temporary\n\n**Current Setup:**\n‚Ä¢ LM Studio URL: `{}`\n‚Ä¢ Model: `{}`\n\n*Source: <{}>*", 
                            e, config.base_url, selected_model, url)
                };
                
                msg.edit(ctx, |m| m.content(&error_message)).await?;
                return Ok(());
            }
        };
    
    debug!("üì° === STREAMING RESPONSE RECEIVED ===");
    debug!("üì° HTTP Response Status: {}", response.status());
    debug!("üì° HTTP Response Status Code: {}", response.status().as_u16());
    debug!("üì° HTTP Response Success: {}", response.status().is_success());
    trace!("üîç Streaming response received: status={}, success={}, stream_uuid={}", 
           response.status(), response.status().is_success(), stream_uuid);
    
    if !response.status().is_success() {
        error!("‚ùå === STREAMING API ERROR ===");
        error!("‚ùå API request failed: HTTP {}", response.status());
        debug!("üîç API error details: status_code={}, status_text={}", response.status().as_u16(), response.status().as_str());
        trace!("üîç API request failed: status={}, status_code={}, stream_uuid={}", 
               response.status(), response.status().as_u16(), stream_uuid);
        return Err(format!("API returned error: {}", response.status()).into());
    }
    
    debug!("‚úÖ API request successful: HTTP {}", response.status());
    trace!("üîç API request successful: status={}, stream_uuid={}", response.status(), stream_uuid);
    
    debug!("üì° === STREAMING PROCESSING ===");
    debug!("üì° Starting to process streaming response...");
    trace!("üîç Streaming processing started: stream_uuid={}", stream_uuid);
    
    let mut accumulated = String::new();
    let start_time = Instant::now();
    let mut last_update = Instant::now();
    let mut chunk_count = 0;
    
    debug!("üìä === STREAMING STATISTICS INITIALIZATION ===");
    debug!("üìä Start time: {:?}", start_time);
    debug!("üìä Last update time: {:?}", last_update);
    debug!("üìä Initial chunk count: {}", chunk_count);
    trace!("üîç Streaming started: start_time={:?}, stream_uuid={}", start_time, stream_uuid);
    
    while let Some(chunk) = match response.chunk().await {
        Ok(chunk_opt) => chunk_opt,
        Err(e) => {
            error!("‚ùå === STREAMING CHUNK ERROR ===");
            error!("‚ùå Failed to read streaming chunk: {}", e);
            
            let error_message = format!(
                "‚ùå **Streaming Error**\n\nFailed to read streaming response: {}\n\n**Solutions:**\n‚Ä¢ Try again with a shorter video/webpage\n‚Ä¢ Check your internet connection\n‚Ä¢ Verify AI model is stable\n\n*Source: <{}>*", 
                e, url
            );
            
            msg.edit(ctx, |m| m.content(&error_message)).await?;
            return Ok(());
        }
    } {
        chunk_count += 1;
        debug!("üì° === CHUNK {} RECEIVED ===", chunk_count);
        debug!("üì° Received chunk {}: {} bytes", chunk_count, chunk.len());
        trace!("üîç Received chunk {}: size={} bytes, stream_uuid={}", chunk_count, chunk.len(), stream_uuid);
        
        let chunk_str = String::from_utf8_lossy(&chunk);
        debug!("üì° Chunk {} as string: {} characters", chunk_count, chunk_str.len());
        debug!("üì° Chunk {} preview: {}", chunk_count, &chunk_str[..std::cmp::min(100, chunk_str.len())]);
        
        for (line_num, line) in chunk_str.lines().enumerate() {
            debug!("üìù === LINE {} PROCESSING ===", line_num + 1);
            debug!("üìù Processing line: '{}'", line);
            
            if line.starts_with("data: ") {
                let data = &line[6..];
                debug!("üìù Found data line: {} characters", data.len());
                debug!("üìù Data content: '{}'", data);
                
                if data == "[DONE]" {
                    debug!("‚úÖ === STREAM COMPLETION ===");
                    debug!("‚úÖ Received [DONE] signal, ending stream");
                    trace!("üîç Received [DONE] signal, ending stream: stream_uuid={}", stream_uuid);
                    break;
                }
                
                match serde_json::from_str::<StreamResponse>(data) {
                    Ok(stream_resp) => {
                        debug!("‚úÖ === STREAM RESPONSE PARSED ===");
                        debug!("‚úÖ Successfully parsed stream response");
                        debug!("‚úÖ Choices count: {}", stream_resp.choices.len());
                        
                        if let Some(choice) = stream_resp.choices.get(0) {
                            if let Some(content) = &choice.delta.content {
                                debug!("üìù === CONTENT CHUNK ADDED ===");
                                debug!("üìù Adding content chunk: {} characters", content.len());
                                debug!("üìù Content chunk: '{}'", content);
                                accumulated.push_str(content);
                                debug!("üìù Total accumulated: {} characters", accumulated.len());
                                trace!("üîç Added content chunk: length={}, total_accumulated={}, stream_uuid={}", 
                                       content.len(), accumulated.len(), stream_uuid);
                            }
                            if choice.finish_reason.is_some() {
                                debug!("‚úÖ === STREAM FINISHED ===");
                                debug!("‚úÖ Received finish_reason: {:?}", choice.finish_reason);
                                trace!("üîç Received finish_reason: {:?}, ending stream: stream_uuid={}", 
                                       choice.finish_reason, stream_uuid);
                                break;
                            }
                        }
                    }
                    Err(e) => {
                        debug!("‚ùå === STREAM PARSE ERROR ===");
                        debug!("‚ùå Failed to parse stream response: {}", e);
                        debug!("‚ùå Raw data: '{}'", data);
                        trace!("üîç Failed to parse stream response: error={}, data={}, stream_uuid={}", 
                               e, data, stream_uuid);
                    }
                }
            } else {
                debug!("üìù Line does not start with 'data: ', skipping");
                trace!("üîç Skipped non-data line: line={}, stream_uuid={}", line, stream_uuid);
            }
        }
        
        // Periodic update to Discord every 5 seconds
        if last_update.elapsed() > Duration::from_secs(5) {
            let elapsed = start_time.elapsed().as_secs();
            debug!("‚è∞ === PERIODIC DISCORD UPDATE ===");
            debug!("‚è∞ Periodic Discord update: {} seconds elapsed", elapsed);
            debug!("‚è∞ Accumulated content: {} characters", accumulated.len());
            trace!("üîç Periodic Discord update: elapsed_seconds={}, accumulated_length={}, stream_uuid={}", 
                   elapsed, accumulated.len(), stream_uuid);
            
            msg.edit(ctx, |m| m.content(format!("ü§ñ Generating summary... ({}s)", elapsed))).await?;
            last_update = Instant::now();
            debug!("‚úÖ Discord message updated successfully");
        }
    }
    
    debug!("üìä === STREAMING COMPLETED ===");
    debug!("üìä Total chunks received: {}", chunk_count);
    debug!("üìä Total streaming time: {:.2}s", start_time.elapsed().as_secs_f64());
    debug!("üìä Final accumulated content: {} characters", accumulated.len());
    trace!("üîç Streaming completed: chunk_count={}, total_time={:.2}s, final_length={}, stream_uuid={}", 
           chunk_count, start_time.elapsed().as_secs_f64(), accumulated.len(), stream_uuid);
    
    // Strip <think> sections from full accumulated response (normal for reasoning models)
    debug!("üßπ === THINKING TAG REMOVAL ===");
    debug!("üßπ Removing <think> tags from accumulated content...");
    let before_stripping = accumulated.len();
    
    let re = Regex::new(r"(?s)<think>.*?</think>").unwrap();
    let stripped = re.replace_all(&accumulated, "").to_string();
    
    debug!("‚úÖ === THINKING TAG REMOVAL COMPLETED ===");
    debug!("‚úÖ Thinking tag removal completed");
    debug!("üìä Before stripping: {} characters", before_stripping);
    debug!("üìä After stripping: {} characters", stripped.len());
    debug!("üìä Stripping reduction: {:.2}%", (stripped.len() as f64 / before_stripping as f64) * 100.0);
    debug!("üìä Content preview: {}", &stripped[..std::cmp::min(300, stripped.len())]);
    trace!("üîç Content processing: original_accumulated={}, stripped_length={}, chunk_count={}, stream_uuid={}", 
           accumulated.len(), stripped.len(), chunk_count, stream_uuid);
    
    // Check if the model returned a fallback message
    debug!("üîç === FALLBACK MESSAGE CHECK ===");
    debug!("üîç Checking for fallback messages in final response...");
    debug!("üîç Contains 'Search functionality is not available': {}", stripped.contains("Search functionality is not available"));
    debug!("üîç Contains 'fallback': {}", stripped.contains("fallback"));
    
    if stripped.contains("Search functionality is not available") || stripped.contains("fallback") {
        warn!("‚ö†Ô∏è === FALLBACK MESSAGE DETECTED ===");
        warn!("‚ö†Ô∏è Model {} returned fallback message, indicating it's not suitable for summarization", selected_model);
        debug!("üîç Final response contains fallback message: {}", stripped);
        trace!("üîç Fallback message detected: model={}, stream_uuid={}", selected_model, stream_uuid);
        
        // Provide a user-friendly error message
        let error_message = format!(
            "‚ùå **Summarization Failed**\n\n**Issue:** The AI model `{}` appears to be unsuitable for content summarization.\n\n**Solution:** This may indicate:\n‚Ä¢ **Model Type**: Model may need different prompting strategies\n‚Ä¢ **Content Complexity**: Try shorter content or different approach\n‚Ä¢ **Model Configuration**: Verify model is properly loaded in LM Studio\n\n**Alternative:** Try using a different model configured for summarization.\n\n*Source: <{}>*",
            selected_model, url
        );
        
        debug!("üìù === FALLBACK ERROR MESSAGE ===");
        debug!("üìù Sending fallback error message to Discord...");
        msg.edit(ctx, |m| m.content(&error_message)).await?;
        debug!("‚úÖ Fallback error message sent successfully");
        trace!("üîç Fallback error message sent: stream_uuid={}", stream_uuid);
        return Ok(());
    }
    
    // Check if we got meaningful content
    debug!("üîç === CONTENT VALIDATION ===");
    debug!("üîç Validating final content...");
    debug!("üîç Content length: {} characters", stripped.len());
    debug!("üîç Content is empty: {}", stripped.trim().is_empty());
    debug!("üîç Content is too short: {}", stripped.len() < 50);
    
    if stripped.trim().is_empty() || stripped.len() < 50 {
        error!("‚ùå === INSUFFICIENT CONTENT ERROR ===");
        error!("‚ùå LLM returned insufficient content: {} characters", stripped.len());
        debug!("üîç Insufficient content: length={}, content='{}'", stripped.len(), stripped);
        trace!("üîç Insufficient content: length={}, stream_uuid={}", stripped.len(), stream_uuid);
        
        let error_message = format!(
            "‚ùå **Summarization Failed**\n\n**Issue:** The AI model returned insufficient content ({} characters).\n\n**Possible causes:**\n‚Ä¢ Model is not properly configured for summarization\n‚Ä¢ Content was too long or complex\n‚Ä¢ API connection issues\n\n*Source: <{}>*",
            stripped.len(), url
        );
        
        debug!("üìù === INSUFFICIENT CONTENT ERROR MESSAGE ===");
        debug!("üìù Sending insufficient content error message to Discord...");
        msg.edit(ctx, |m| m.content(&error_message)).await?;
        debug!("‚úÖ Insufficient content error message sent successfully");
        trace!("üîç Insufficient content error message sent: stream_uuid={}", stream_uuid);
        return Ok(());
    }
    
    // Final update
    debug!("üìù === FINAL MESSAGE CREATION ===");
    debug!("üìù Creating final Discord message...");
    
    let final_message = format!(
        "**{} Summary**\n\n{}\n\n*Source: <{}>*",
        if is_youtube { "YouTube Video" } else { "Webpage" },
        stripped.trim(),
        url
    );
    
    debug!("üìù Final message created: {} characters", final_message.len());
    debug!("üìù Final message preview: {}", &final_message[..std::cmp::min(300, final_message.len())]);
    trace!("üîç Final message created: length={}, is_youtube={}, stream_uuid={}", 
           final_message.len(), is_youtube, stream_uuid);
    
    // Split if too long
    let max_length = config.max_discord_message_length - config.response_format_padding;
    debug!("üìè === MESSAGE LENGTH CHECK ===");
    debug!("üìè Final message length: {} characters", final_message.len());
    debug!("üìè Max Discord message length: {}", config.max_discord_message_length);
    debug!("üìè Response format padding: {}", config.response_format_padding);
    debug!("üìè Effective max length: {} characters", max_length);
    debug!("üìè Needs splitting: {}", final_message.len() > max_length);
    trace!("üîç Message length check: final_length={}, max_length={}, needs_splitting={}, stream_uuid={}", 
           final_message.len(), max_length, final_message.len() > max_length, stream_uuid);
    
    if final_message.len() > max_length {
        info!("üìÑ === MESSAGE SPLITTING ===");
        info!("üìÑ Message too long, splitting into chunks...");
        debug!("üìÑ Original message length: {} characters", final_message.len());
        debug!("üìÑ Max chunk length: {} characters", max_length);
        trace!("üîç Message splitting started: original_length={}, max_chunk_length={}, stream_uuid={}", 
               final_message.len(), max_length, stream_uuid);
        
        let chunks = split_message(&final_message, max_length);
        debug!("üìÑ Split into {} chunks", chunks.len());
        debug!("üìÑ Chunk sizes: {:?}", chunks.iter().map(|c| c.len()).collect::<Vec<_>>());
        trace!("üîç Message split completed: chunk_count={}, stream_uuid={}", chunks.len(), stream_uuid);
        
        for (i, chunk) in chunks.iter().enumerate() {
            debug!("üì§ === SENDING CHUNK {} ===", i+1);
            debug!("üì§ Sending chunk {}: {} characters", i+1, chunk.len());
            trace!("üîç Sending chunk {}: length={}, stream_uuid={}", i+1, chunk.len(), stream_uuid);
            
            if i == 0 {
                debug!("üì§ Sending first chunk via edit");
                msg.edit(ctx, |m| m.content(chunk)).await?;
                trace!("üîç First chunk sent via edit: stream_uuid={}", stream_uuid);
            } else {
                debug!("üì§ Sending additional chunk {} via new message", i+1);
                msg.channel_id.say(ctx, chunk).await?;
                trace!("üîç Additional chunk {} sent via new message: stream_uuid={}", i+1, stream_uuid);
            }
            debug!("‚úÖ Chunk {} sent successfully", i+1);
        }
    } else {
        debug!("üì§ === SENDING SINGLE MESSAGE ===");
        debug!("üì§ Sending single message: {} characters", final_message.len());
        trace!("üîç Sending single message: length={}, stream_uuid={}", final_message.len(), stream_uuid);
        
        msg.edit(ctx, |m| m.content(&final_message)).await?;
        debug!("‚úÖ Single message sent successfully");
        trace!("üîç Single message sent successfully: stream_uuid={}", stream_uuid);
    }
    
    info!("‚úÖ === AI SUMMARIZATION STREAMING COMPLETED ===");
    info!("‚úÖ Stream summary completed successfully");
    debug!("üìä Final statistics:");
    debug!("üìä   - Stream UUID: {}", stream_uuid);
    debug!("üìä   - Total chunks received: {}", chunk_count);
    debug!("üìä   - Total streaming time: {:.2}s", start_time.elapsed().as_secs_f64());
    debug!("üìä   - Final content length: {} characters", stripped.len());
    debug!("üìä   - Final message length: {} characters", final_message.len());
    debug!("üìä   - Content type: {}", if is_youtube { "YouTube" } else { "Webpage" });
    trace!("üîç Stream summary completed successfully: stream_uuid={}", stream_uuid);
    
    Ok(())
}

// Split long messages into Discord-sized chunks
// Used to avoid exceeding Discord's message length limit
fn split_message(content: &str, max_len: usize) -> Vec<String> {
    let split_uuid = Uuid::new_v4();
    
    debug!("üìÑ === MESSAGE SPLITTING STARTED ===");
    debug!("üÜî Split UUID: {}", split_uuid);
    debug!("üìÑ Original content length: {} characters", content.len());
    debug!("üìÑ Max chunk length: {} characters", max_len);
    debug!("üìÑ Needs splitting: {}", content.len() > max_len);
    trace!("üîç Message splitting started: content_length={}, max_len={}, split_uuid={}", 
           content.len(), max_len, split_uuid);
    
    let mut chunks = Vec::new();
    let mut current = String::new();
    let mut line_count = 0;
    let mut chunk_count = 0;
    
    debug!("üìù === LINE PROCESSING ===");
    debug!("üìù Processing content line by line...");
    
    for (line_num, line) in content.lines().enumerate() {
        line_count += 1;
        debug!("üìù === LINE {} PROCESSING ===", line_num + 1);
        debug!("üìù Line {} length: {} characters", line_num + 1, line.len());
        debug!("üìù Line {} content: '{}'", line_num + 1, line);
        debug!("üìù Current chunk length: {} characters", current.len());
        debug!("üìù Would exceed limit: {}", current.len() + line.len() + 1 > max_len);
        
        if current.len() + line.len() + 1 > max_len && !current.is_empty() {
            chunk_count += 1;
            debug!("üìÑ === CHUNK {} CREATED ===", chunk_count);
            debug!("üìÑ Creating chunk {}: {} characters", chunk_count, current.len());
            debug!("üìÑ Chunk {} content: '{}'", chunk_count, current.trim());
            chunks.push(current.trim().to_string());
            trace!("üîç Chunk {} created: length={}, split_uuid={}", chunk_count, current.len(), split_uuid);
            current = String::new();
            debug!("üìÑ Reset current chunk for next content");
        }
        
        if !current.is_empty() {
            debug!("üìù Adding newline to current chunk");
            current.push('\n');
        }
        
        debug!("üìù Adding line {} to current chunk", line_num + 1);
        current.push_str(line);
        debug!("üìù Current chunk after adding line: {} characters", current.len());
        trace!("üîç Line {} processed: line_length={}, current_chunk_length={}, split_uuid={}", 
               line_num + 1, line.len(), current.len(), split_uuid);
    }
    
    if !current.is_empty() {
        chunk_count += 1;
        debug!("üìÑ === FINAL CHUNK {} CREATED ===", chunk_count);
        debug!("üìÑ Creating final chunk {}: {} characters", chunk_count, current.len());
        debug!("üìÑ Final chunk content: '{}'", current.trim());
        chunks.push(current.trim().to_string());
        trace!("üîç Final chunk {} created: length={}, split_uuid={}", chunk_count, current.len(), split_uuid);
    }
    
    debug!("‚úÖ === MESSAGE SPLITTING COMPLETED ===");
    debug!("‚úÖ Message splitting completed successfully");
    debug!("üìä Final statistics:");
    debug!("üìä   - Split UUID: {}", split_uuid);
    debug!("üìä   - Total lines processed: {}", line_count);
    debug!("üìä   - Total chunks created: {}", chunks.len());
    debug!("üìä   - Original content length: {} characters", content.len());
    debug!("üìä   - Total chunked content length: {} characters", chunks.iter().map(|c| c.len()).sum::<usize>());
    debug!("üìä   - Chunk sizes: {:?}", chunks.iter().map(|c| c.len()).collect::<Vec<_>>());
    debug!("üìä   - Efficiency: {:.2}%", (chunks.iter().map(|c| c.len()).sum::<usize>() as f64 / content.len() as f64) * 100.0);
    
    trace!("üîç Message splitting completed: line_count={}, chunk_count={}, original_length={}, total_chunked_length={}, split_uuid={}", 
           line_count, chunks.len(), content.len(), chunks.iter().map(|c| c.len()).sum::<usize>(), split_uuid);
    
    chunks
}

// ============================================================================
// COMMAND GROUP
// ============================================================================

// Commands are auto-registered by the #[command] macro

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_clean_vtt() {
        let vtt = r#"WEBVTT

00:00:00.000 --> 00:00:03.000
Hello world

00:00:03.000 --> 00:00:06.000
This is a test"#;
        
        let cleaned = clean_vtt_content(vtt);
        assert_eq!(cleaned, "Hello world This is a test");
    }
    
    #[test]
    fn test_clean_html() {
        let html = "<p>Hello <b>world</b></p><script>alert('test');</script>";
        let cleaned = clean_html(html);
        assert_eq!(cleaned, "Hello world");
    }
    
    #[test]
    fn test_webpage_content_processing() {
        // Test that web page content is processed correctly
        let test_content = "This is a test webpage content with some information about various topics. It contains multiple sentences and should be processed properly for summarization.";
        
        // Simulate the content processing logic
        let max_content_length = 20000;
        let truncated_content = if test_content.len() > max_content_length {
            format!("{} [Content truncated due to length]", &test_content[0..max_content_length])
        } else {
            test_content.to_string()
        };
        
        // Verify content is not truncated
        assert_eq!(truncated_content, test_content);
        
        // Test chunking logic
        let chunk_size = 8000;
        let mut chunks = Vec::new();
        let mut current_chunk = String::new();
        
        for word in truncated_content.split_whitespace() {
            if current_chunk.len() + word.len() + 1 > chunk_size && !current_chunk.is_empty() {
                chunks.push(current_chunk.trim().to_string());
                current_chunk = String::new();
            }
            
            if !current_chunk.is_empty() {
                current_chunk.push(' ');
            }
            current_chunk.push_str(word);
        }
        
        if !current_chunk.is_empty() {
            chunks.push(current_chunk.trim().to_string());
        }
        
        // Verify chunking works correctly
        assert_eq!(chunks.len(), 1); // Should fit in one chunk
        assert_eq!(chunks[0], test_content);
    }
    
    #[test]
    fn test_html_file_processing() {
        // Test HTML cleaning functionality
        let test_html = r#"<html><head><title>Test Page</title></head><body><h1>Hello World</h1><p>This is a <b>test</b> paragraph.</p><script>alert('test');</script><style>body { color: red; }</style></body></html>"#;
        
        let cleaned = clean_html(test_html);
        
        // Should contain the text content but not HTML tags, scripts, or styles
        assert!(cleaned.contains("Hello World"));
        assert!(cleaned.contains("This is a test paragraph"));
        assert!(!cleaned.contains("<script>"));
        assert!(!cleaned.contains("<style>"));
        assert!(!cleaned.contains("<html>"));
        assert!(!cleaned.contains("<b>"));
    }
    
    #[test]
    fn test_lm_config_structure() {
        // Test that the LMConfig structure can be created and has all expected fields
        let config = LMConfig {
            base_url: "http://localhost:1234".to_string(),
            timeout: 60,
            default_model: "test-model".to_string(),
            default_reason_model: "test-reason-model".to_string(),
            default_summarization_model: "test-sum-model".to_string(),
            default_ranking_model: "test-rank-model".to_string(),
            default_temperature: 0.7,
            default_max_tokens: 2000,
            max_discord_message_length: 2000,
            response_format_padding: 100,
            default_vision_model: "test-vision-model".to_string(),
            default_seed: Some(42),
        };
        
        assert_eq!(config.base_url, "http://localhost:1234");
        assert_eq!(config.timeout, 60);
        assert_eq!(config.default_model, "test-model");
        assert_eq!(config.default_summarization_model, "test-sum-model");
        assert_eq!(config.default_temperature, 0.7);
        assert_eq!(config.default_max_tokens, 2000);
        assert_eq!(config.default_seed, Some(42));
    }
    
    #[test]
    fn test_chat_message_structure() {
        // Test that the ChatMessage structure can be created and serialized
        let message = ChatMessage {
            role: "user".to_string(),
            content: "Hello, world!".to_string(),
        };
        
        assert_eq!(message.role, "user");
        assert_eq!(message.content, "Hello, world!");
        
        // Test serialization
        let json = serde_json::to_string(&message).unwrap();
        assert!(json.contains("user"));
        assert!(json.contains("Hello, world!"));
        
        // Test deserialization
        let deserialized: ChatMessage = serde_json::from_str(&json).unwrap();
        assert_eq!(deserialized.role, "user");
        assert_eq!(deserialized.content, "Hello, world!");
    }
    
    #[test]
    fn test_split_message_functionality() {
        // Test the message splitting functionality with multi-line content
        // The split_message function splits by lines, not character count
        let lines = vec![
            "This is line 1 with some text that makes it moderately long to test splitting.",
            "This is line 2 with additional content that should also be quite lengthy.",
            "This is line 3 with even more text to ensure we exceed the character limit.",
            "This is line 4 which continues to add content for comprehensive testing.",
            "This is line 5 that should definitely push us over the max_len limit.",
            "This is line 6 with final content to ensure proper multi-chunk splitting.",
            "This is line 7 with additional verification content for thorough testing.",
            "This is line 8 providing the last bit of content for split verification."
        ];
        let long_content = lines.join("\n");
        let max_len = 200; // Small limit to force splitting
        
        // Verify the content is actually long enough to require splitting
        assert!(long_content.len() > max_len, "Test content should be longer than max_len for proper testing");
        
        let chunks = split_message(&long_content, max_len);
        
        // Should create multiple chunks since we have multiple lines that exceed the limit
        assert!(chunks.len() > 1, "Content of {} chars with {} lines should split with max_len {}", 
                long_content.len(), lines.len(), max_len);
        
        // Each chunk should be within the limit (except possibly the last one with remaining content)
        for (i, chunk) in chunks.iter().enumerate() {
            if i < chunks.len() - 1 {
                assert!(chunk.len() <= max_len, "Chunk {} exceeds max length: {} > {}", i, chunk.len(), max_len);
            }
        }
        
        // All chunks combined should contain the original content (minus whitespace differences)
        let combined = chunks.join("\n");
        let original_words: Vec<&str> = long_content.split_whitespace().collect();
        let combined_words: Vec<&str> = combined.split_whitespace().collect();
        assert_eq!(original_words.len(), combined_words.len());
        
        // Test with very short content that shouldn't be split
        let short_content = "Short message";
        let short_chunks = split_message(short_content, max_len);
        assert_eq!(short_chunks.len(), 1, "Short content should not be split");
        assert_eq!(short_chunks[0], short_content);
        
        // Test with single long line (should not be split since split_message works by lines)
        let single_long_line = "This is a very long single line that exceeds the max_len but has no line breaks so it cannot be split by the line-based splitting algorithm.".repeat(5);
        let single_line_chunks = split_message(&single_long_line, max_len);
        assert_eq!(single_line_chunks.len(), 1, "Single long line should not be split");
    }
}

// Command group exports
#[group]
#[commands(sum)]
pub struct Sum;

impl Sum {
    pub const fn new() -> Self {
        Sum
    }
} 