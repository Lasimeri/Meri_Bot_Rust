# LM Studio/Ollama API Configuration
# Copy this file to lmapiconf.txt and modify ALL settings as needed
# ALL SETTINGS BELOW ARE MANDATORY - no defaults are provided

# Server configuration
# LM Studio: http://localhost:1234
# Ollama: http://localhost:11434 or http://127.0.0.1:11434
LM_STUDIO_BASE_URL=http://127.0.0.1:11434
LM_STUDIO_TIMEOUT=30

# Model configuration
# Replace with your actual model name from LM Studio/Ollama
# Examples: llama3.1:8b, codellama:13b, mistral:7b
DEFAULT_MODEL=darkidol-llama-3.1-8b-instruct-1.2-uncensored@q2_k

# Chat completion parameters
DEFAULT_TEMPERATURE=0.7
DEFAULT_MAX_TOKENS=2048

# Discord integration settings
MAX_DISCORD_MESSAGE_LENGTH=2000
RESPONSE_FORMAT_PADDING=50 