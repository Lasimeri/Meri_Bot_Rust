# LM Studio/Ollama API Configuration
# Copy this file to lmapiconf.txt and modify ALL settings as needed
# ALL SETTINGS BELOW ARE MANDATORY - no defaults are provided

# Server configuration
# LM Studio: http://localhost:1234
# Ollama: http://localhost:11434 or http://127.0.0.1:11434
# Remote server example: http://192.168.0.87:11434
LM_STUDIO_BASE_URL=http://127.0.0.1:11434
LM_STUDIO_TIMEOUT=30

# Model configuration
# Replace with your actual model name from LM Studio/Ollama
# Examples: llama3.1:8b, codellama:13b, mistral:7b, qwen:7b
DEFAULT_MODEL=your-chat-model-name

# Reasoning model configuration (for ^reason command)
# Use a model specifically designed for reasoning tasks
# Examples: qwen2.5:14b, deepseek-r1:8b, llama3.1:8b-reasoning
DEFAULT_REASON_MODEL=deepseek/deepseek-r1-0528-qwen3-8b

# Chat completion parameters
DEFAULT_TEMPERATURE=0.8
DEFAULT_MAX_TOKENS=8192

# Discord integration settings
MAX_DISCORD_MESSAGE_LENGTH=2000
RESPONSE_FORMAT_PADDING=100

# SerpAPI Configuration (Required - for search functionality)
# Get your free API key from https://serpapi.com/users/sign_up
# This is the ONLY search method supported
SERPAPI_ENGINE=google
SERPAPI_COUNTRY=us
SERPAPI_LANGUAGE=en 