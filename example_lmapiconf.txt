# LM Studio/Ollama API Configuration
# Copy this file to lmapiconf.txt and modify ALL settings as needed
# ALL SETTINGS BELOW ARE MANDATORY - no defaults are provided

# Server configuration
# LM Studio: http://localhost:1234
# Ollama: http://localhost:11434 or http://127.0.0.1:11434
LM_STUDIO_BASE_URL=http://127.0.0.1:11434
LM_STUDIO_TIMEOUT=30

# Model configuration
# Replace with your actual model name from LM Studio/Ollama
# Examples: llama3.1:8b, codellama:13b, mistral:7b, qwen:7b
DEFAULT_MODEL=your-chat-model-name

# Reasoning model configuration (for ^reason command)
# Use a model specifically designed for reasoning tasks
# Examples: qwen2.5:14b, deepseek-r1:8b, llama3.1:8b-reasoning
DEFAULT_REASON_MODEL=your-reasoning-model-name

# Chat completion parameters
DEFAULT_TEMPERATURE=0.8
DEFAULT_MAX_TOKENS=8192

# Discord integration settings
MAX_DISCORD_MESSAGE_LENGTH=2000
RESPONSE_FORMAT_PADDING=100 